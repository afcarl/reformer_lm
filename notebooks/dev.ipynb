{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 854,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import jax\n",
    "\n",
    "from trax import backend\n",
    "from trax import layers as tl\n",
    "from trax.backend import numpy as np\n",
    "from trax.layers.combinators import _pop_rng_and_split\n",
    "from trax.shapes import signature, ShapeDtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pMap(nn.Module):\n",
    "    \"\"\"Combinator for applying a layers to a list/tuple\"\"\"\n",
    "    def __init__(self, layer, n_sections=1, check_shapes=True):\n",
    "        \"\"\"\n",
    "        \n",
    "        :param layers: the layer you wish to apply to each element.\n",
    "        :param n_sections: number of sections to map to. defaults to 1.\n",
    "        :param check_shapes: tests to see the shapes are identical.\n",
    "        :returns: new layers with mapped layer to all elements.\n",
    "        \"\"\"\n",
    "        super(pMap, self).__init__()\n",
    "        if layer is None or isinstance(layer, (list, tuple)):\n",
    "            layer = nn.Sequential(*layer)\n",
    "        self._layer = layer\n",
    "        self._check_shapes = check_shapes\n",
    "        self._n_sections = n_sections\n",
    "        self.n_in = n_sections\n",
    "        self.n_out = n_sections\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs, **kwargs):\n",
    "        \"\"\"\n",
    "        Trying to replace with a basic forward pass. Trax implementation\n",
    "        uses a PRNG key split into subsections zipped with the inputs var\n",
    "        then returns a list of those forward passed subsections as results\n",
    "        \"\"\"\n",
    "        \n",
    "        if self._n_sections == 1:\n",
    "            results = self._layer(inputs, **kwargs)\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            results = [self._layer(x) for x in inputs] \n",
    "            \n",
    "        return results, self._layer.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pMap(\n",
       "  (_layer): Sequential(\n",
       "    (0): LayerNorm((1,), eps=1, elementwise_affine=True)\n",
       "    (1): Linear(in_features=1, out_features=1, bias=True)\n",
       "    (2): LogSoftmax()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmap = pMap(layer=[\n",
    "    nn.LayerNorm(1,1), \n",
    "    nn.Linear(1,1),\n",
    "    nn.LogSoftmax()], n_sections=10)\n",
    "\n",
    "pmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n",
      "tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])\n"
     ]
    }
   ],
   "source": [
    "test = torch.tensor([0,1,2,3,4,5,6,7,8,9], dtype=torch.float)\n",
    "print(test.unsqueeze(0).shape)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]], grad_fn=<LogSoftmaxBackward>),\n",
       " OrderedDict([('0.weight', tensor([1.])),\n",
       "              ('0.bias', tensor([0.])),\n",
       "              ('1.weight', tensor([[0.4920]])),\n",
       "              ('1.bias', tensor([0.9482]))]))"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmap(inputs=test.view(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pBroadcastedDropout(nn.Module):\n",
    "    \n",
    "    def __init__(self, rate=0.0, mode='train', broadcast_dims=(-2,)):\n",
    "        super(pBroadcastedDropout, self).__init__()\n",
    "        \n",
    "        self.rate = rate\n",
    "        if self.rate >= 1.0:\n",
    "            raise ValueError(f'Dropout rate ({self.rate}) must be lower than 1')\n",
    "        elif self.rate < 0:\n",
    "            raise ValueError(f'Dropout rate ({self.rate}) must be at least 0.0')\n",
    "        \n",
    "        self.broadcast_dims = broadcast_dims\n",
    "        self.mode = mode\n",
    "        \n",
    "    def forward(self, x: torch.tensor, **kwargs):\n",
    "        if self.mode == 'train' and self.rate > 0.0:\n",
    "            noise_shape = list(x.shape)\n",
    "            \n",
    "            for dim in self.broadcast_dims:\n",
    "                noise_shape[dim] = 1\n",
    "                \n",
    "            keep_prob = 1 - self.rate\n",
    "            keep = np.random.binomial(n=1, p=keep_prob, size=tuple(noise_shape))\n",
    "            keep = torch.tensor(keep)\n",
    "            multiplier = keep / keep_prob\n",
    "            return x * multiplier\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd = pBroadcastedDropout(mode='train', rate=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  1.2500,  2.5000,  3.7500,  5.0000,  6.2500,  7.5000,  8.7500,\n",
       "         10.0000,  0.0000]])"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bd(test.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FeedForward(sample, \n",
    "                d_input: int, \n",
    "                d_output: int, \n",
    "                dropout: float, \n",
    "                activation: bool, \n",
    "                mode: str):\n",
    "    \"\"\"\n",
    "    Building a simple Feed Forward NN\n",
    "    :param sample: sample data used to set the LayerNorm dimensions.\n",
    "    :param d_input: model input dimension.\n",
    "    :param d_output: model output dimension.\n",
    "    :param dropout: amount of dropout to apply on range [0.0, 1.0).\n",
    "    :param activation: True applies ReLU activation.\n",
    "    :param mode: whether to run dropout in train or eval mode.\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(sample.shape) == 1:\n",
    "        norm_output = sample.shape[0]\n",
    "        norm_input = 1\n",
    "    elif len(sample.shape) == 2:\n",
    "        norm_output = sample.shape[1]\n",
    "        norm_input = sample.shape[0]\n",
    "    else:\n",
    "        norm_output = sample.shape[-1]\n",
    "        norm_input = sample.shape[-2]\n",
    "        \n",
    "    \n",
    "    return nn.Sequential(\n",
    "        nn.LayerNorm(normalized_shape=(norm_input, norm_output)),\n",
    "        nn.Linear(d_input, d_output),\n",
    "        pBroadcastedDropout(rate=dropout, mode=mode),\n",
    "        activation,\n",
    "        nn.Linear(d_output, d_output),\n",
    "        pBroadcastedDropout(rate=dropout, mode=mode)\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6268, -0.0000,  0.7972,  0.0000, -1.2683,  0.0000, -0.3375,  0.1356,\n",
       "          0.0523, -0.1159]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff = FeedForward(sample=test.view(1,-1), \n",
    "                 d_input=10, \n",
    "                 d_output=10, \n",
    "                 dropout=0.2, \n",
    "                 activation=nn.ReLU(), \n",
    "                 mode='train')\n",
    "ff(test.view(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitForOutput(nn.Module):\n",
    "    \"\"\"\n",
    "    Splits activations into sections, to be used prior to the output layer.\n",
    "    \n",
    "    After the reversible portion of the network, there is a final output portion that's \n",
    "    non-reversible where the minimum includes normalization, output projection, and log-softmax. \n",
    "    The output portion needs to operate on chucks of the sequence to avoid running out of memory\n",
    "    for large vocabulary sizes.\n",
    "    \n",
    "    This layer concatenates the two subparts of the activations along the feature dimension\n",
    "    then splits into chunks along the time dimension. \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_sections=2, axis=-2, n_in=2):\n",
    "        super(SplitForOutput, self).__init__()\n",
    "        self.n_sections = n_sections\n",
    "        self.axis = axis\n",
    "        self.n_in = 2\n",
    "        self.n_out = n_sections\n",
    "            \n",
    "    def forward(self, inputs: torch.tensor):\n",
    "                    \n",
    "        x1, x2 = inputs\n",
    "\n",
    "        x1_split = np.split(x1, self.n_sections, self.axis)\n",
    "        x2_split = np.split(x2, self.n_sections, self.axis)\n",
    "\n",
    "        res = [np.concatenate(ys, -1) for ys in zip(x1_split, x2_split)]\n",
    "        return tuple(res)\n",
    "\n",
    "    def reverse(self, output, **kwargs):\n",
    "        \n",
    "        x1_split = []\n",
    "        x2_split = []\n",
    "        for y in output:\n",
    "            y1, y2 = np.split(y, 2, -1)\n",
    "            x1_split.append(y1)\n",
    "            x2_split.append(y2)\n",
    "\n",
    "        x1 = np.concatenate(x1_split, self.axis)\n",
    "        x2 = np.concatenate(x2_split, self.axis)\n",
    "\n",
    "        return (x1, x2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 823,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Chunk(x, n_sections=2):\n",
    "    assert x.shape[1] % n_sections == 0\n",
    "    \n",
    "    return torch.cat(torch.chunk(x, chunks=n_sections, dim=-2))\n",
    "\n",
    "def Unchunk(x, n_sections=2):\n",
    "    assert x.shape[0] % n_sections == 0\n",
    "    \n",
    "    return torch.cat(torch.chunk(x, chunks=n_sections, dim=-3), dim=-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 887,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReversibleHalfResidual(nn.Module):\n",
    "    \n",
    "    def __init__(self, residual_layers):\n",
    "        \n",
    "        x = ([], (x, x))\n",
    "        \n",
    "        self.compute_residual = nn.Sequential(\n",
    "            # (x1_or_y1, x2) -> (x2, x1_or_y1, x2)\n",
    "            \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 888,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual_layers = tl.Serial(tl.Dense(10))\n",
    "\n",
    "compute_residual = tl.Serial(\n",
    "        # (x1_or_y1, x2) -> (x2, x1_or_y1, x2)\n",
    "        tl.Parallel([], tl.Dup()),\n",
    "        tl.Swap(),\n",
    "        tl.Parallel(residual_layers, [], []))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-7., -6., -5., -4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.,  5.,\n",
       "        6.,  7.,  8.], dtype=float32)"
      ]
     },
     "execution_count": 876,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.arange(-7, 9).astype(np.float32)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 899,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-7. -6. -5. -4.]\n",
      " [-3. -2. -1.  0.]\n",
      " [ 1.  2.  3.  4.]\n",
      " [ 5.  6.  7.  8.]]\n"
     ]
    },
    {
     "ename": "LayerError",
     "evalue": "Exception passing through layer Serial (in _forward_internal):\n  layer created in file [...]/<ipython-input-858-6edb9536a799>, line 3\n  layer input shapes: (ShapeDtype{shape:(4,), dtype:float32}, ShapeDtype{shape:(4,), dtype:float32}, ShapeDtype{shape:(4,), dtype:float32}, ShapeDtype{shape:(4,), dtype:float32})\n\n  File [...]/trax/layers/combinators.py, line 75, in forward_with_state\n    '({})'.format(len(state), n_layers))\n\nValueError: length of state (0) not equal to number of layers (2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/trax/layers/base.py\u001b[0m in \u001b[0;36m_forward_internal\u001b[0;34m(self, x, weights, state, rng)\u001b[0m\n\u001b[1;32m    418\u001b[0m         outputs, s = self.forward_with_state(\n\u001b[0;32m--> 419\u001b[0;31m             x, weights=weights, state=state, rng=rng)\n\u001b[0m\u001b[1;32m    420\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/trax/layers/combinators.py\u001b[0m in \u001b[0;36mforward_with_state\u001b[0;34m(self, xs, weights, state, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m       raise ValueError('length of state ({}) not equal to number of layers '\n\u001b[0;32m---> 75\u001b[0;31m                        '({})'.format(len(state), n_layers))\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: length of state (0) not equal to number of layers (2)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLayerError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-899-aabed100646b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/trax/layers/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'state'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0mrng\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rng'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m     \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrng\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/trax/layers/base.py\u001b[0m in \u001b[0;36m_forward_internal\u001b[0;34m(self, x, weights, state, rng)\u001b[0m\n\u001b[1;32m    426\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_short_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m       raise LayerError(name, '_forward_internal',\n\u001b[0;32m--> 428\u001b[0;31m                        self._caller, signature(x), trace)\n\u001b[0m\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_forward_abstract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_signature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLayerError\u001b[0m: Exception passing through layer Serial (in _forward_internal):\n  layer created in file [...]/<ipython-input-858-6edb9536a799>, line 3\n  layer input shapes: (ShapeDtype{shape:(4,), dtype:float32}, ShapeDtype{shape:(4,), dtype:float32}, ShapeDtype{shape:(4,), dtype:float32}, ShapeDtype{shape:(4,), dtype:float32})\n\n  File [...]/trax/layers/combinators.py, line 75, in forward_with_state\n    '({})'.format(len(state), n_layers))\n\nValueError: length of state (0) not equal to number of layers (2)"
     ]
    }
   ],
   "source": [
    "x = t.reshape(4, -1)\n",
    "print(x)\n",
    "y = tls(tuple(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 847,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-847-1fe4c79d1129>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/trax/layers/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, weights)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;34m-\u001b[0m \u001b[0mn_out\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtuple\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mn_out\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \"\"\"\n\u001b[0;32m--> 179\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m   def forward_with_state(self, inputs, weights=EMPTY_WEIGHTS, state=EMPTY_STATE,\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tls.forward(tuple(t), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfo = SplitForOutput()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.rand((4,4))\n",
    "a, b = sfo.forward((test.unsqueeze(0),test.unsqueeze(0)))\n",
    "c, d = sfo.reverse((test.unsqueeze(0),test.unsqueeze(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Map(tl.Layer):\n",
    "  \"\"\"Combinator for applying a layer to a list or tuple.\"\"\"\n",
    "\n",
    "  def __init__(self, layer, n_sections=1, check_shapes=True):\n",
    "    \"\"\"Initialize the combinator.\n",
    "    Args:\n",
    "      layer: a layer to apply to each element.\n",
    "      n_sections: how many sections to map to (default: 1).\n",
    "      check_shapes: whether to check that shapes are identical (default: true).\n",
    "    Returns:\n",
    "      A new layer representing mapping layer to all elements of the input.\n",
    "    \"\"\"\n",
    "    super(Map, self).__init__()#n_in=n_sections, n_out=n_sections)\n",
    "    if layer is None or isinstance(layer, (list, tuple)):\n",
    "        layer = tl.Serial(layer)\n",
    "    self._layer = layer\n",
    "    \n",
    "    # Generally a Map should be applied to lists where all elements have\n",
    "    # the same shape -- because self._layer will only be initialized once\n",
    "    # and it could have different parameters for different shapes. But there\n",
    "    # are valid cases -- e.g., when self._layer has no parameters -- where we\n",
    "    # can apply Map to different shapes -- set check_shapes=False in such cases.\n",
    "    self._check_shapes = check_shapes\n",
    "    self._n_sections = n_sections\n",
    "    self.n_in = n_sections\n",
    "    self.n_out = n_sections\n",
    "\n",
    "    def forward_with_state(self, inputs, weights=(), state=(), **kwargs):\n",
    "      if self._n_sections == 1:\n",
    "        results = self._layer(inputs, weights=weights, state=state, **kwargs)\n",
    "      else:\n",
    "        rngs = _pop_rng_and_split(kwargs, len(inputs))\n",
    "        results = [self._layer(x, weights=weights, state=state, rng=r, **kwargs)\n",
    "                 for x, r in zip(inputs, rngs)]\n",
    "        results = tuple(results)\n",
    "      # TODO(kitaev): think about how to merge state across copies in the map.\n",
    "      return results, self._layer.state\n",
    "\n",
    "  def new_weights_and_state(self, input_signature):\n",
    "    if self._n_sections == 1:\n",
    "      return self._layer.init(input_signature)\n",
    "    first_shape = input_signature[0].shape\n",
    "    if self._check_shapes:\n",
    "      for shape_dtype in input_signature:\n",
    "        if shape_dtype.shape != first_shape:\n",
    "          raise ValueError('Map layer can only be applied to list of elements '\n",
    "                           'with the same shapes. This shape %s vs first shape '\n",
    "                           '%s.' % (str(shape_dtype.shape), str(first_shape)))\n",
    "    return self._layer.init(input_signature[0])\n",
    "\n",
    "  @tl.Layer.weights.setter\n",
    "  def weights(self, weights):\n",
    "    self._weights = self._layer.weights = weights\n",
    "\n",
    "  @tl.Layer.state.setter\n",
    "  def state(self, state):\n",
    "    self._state = self._layer.state = state\n",
    "\n",
    "  def _set_input_signature_recursive(self, input_signature):\n",
    "    self._input_signature = input_signature\n",
    "    self._layer._set_input_signature_recursive(input_signature)  # pylint: disable=protected-access\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BroadcastedDropout(tl.Layer):\n",
    "  \"\"\"Layer constructor function for a broadcasted dropout layer.\"\"\"\n",
    "\n",
    "  def __init__(self, rate=0.0, mode='train', broadcast_dims=(-2,)):\n",
    "    super(BroadcastedDropout, self).__init__()\n",
    "    self._rate = rate\n",
    "    if self._rate >= 1.0:\n",
    "      raise ValueError('Dropout rate (%f) must be lower than 1.' % rate)\n",
    "    self._broadcast_dims = broadcast_dims\n",
    "    self._mode = mode\n",
    "\n",
    "  def forward_with_state(self, x, weights, state, rng):\n",
    "    \"\"\"Dropout, with broadcasting to save memory.\"\"\"\n",
    "    del weights\n",
    "    if rng is None:\n",
    "      raise ValueError('BroadcastedDropout requires rng kwarg.')\n",
    "    if self._mode == 'train' and self._rate > 0.0:\n",
    "      noise_shape = list(x.shape)\n",
    "      for dim in self._broadcast_dims:\n",
    "        noise_shape[dim] = 1\n",
    "      keep_prob = jax.lax.tie_in(rng, 1.0 - self._rate)\n",
    "      keep = backend.random.bernoulli(rng, keep_prob, tuple(noise_shape))\n",
    "      multiplier = keep.astype(x.dtype) / jax.lax.tie_in(keep, keep_prob)\n",
    "      return x * multiplier, state\n",
    "    else:\n",
    "      return x, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 895,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FeedForward(d_model, d_ff, dropout, activation, mode):\n",
    "  \"\"\"Feed-forward block with layer normalization at start.\"\"\"\n",
    "  return [\n",
    "      tl.LayerNorm(),\n",
    "      tl.Dense(d_ff),\n",
    "      BroadcastedDropout(rate=dropout, mode=mode),  # pylint: disable=no-value-for-parameter\n",
    "      activation(),\n",
    "      tl.Dense(d_model),\n",
    "      BroadcastedDropout(rate=dropout, mode=mode),  # pylint: disable=no-value-for-parameter\n",
    "  ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitForOutput(tl.ReversibleLayer):\n",
    "  \"\"\"Splits activations into sections (for use right before the output layer).\n",
    "  After the reversible portion of the network, there is a final output portion\n",
    "  that's non-reversible (which at minimum includes normalization, output\n",
    "  projection, and log-softmax). The output portion needs to operate on chunks\n",
    "  of the sequence to avoid running out of memory for large vocabulary sizes.\n",
    "  This layer concatenates the two subparts of the activations along the feature\n",
    "  dimension, and then splits into chunks along the time dimension. We implement\n",
    "  it is a subclass of tl.ReversibleLayer because we want to ensure that multiple\n",
    "  copies of the activations don't exist simultaneously except in the middle of a\n",
    "  memory copy operation.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, n_sections=2, axis=-2):\n",
    "    super(SplitForOutput, self).__init__(n_in=2, n_out=n_sections)\n",
    "    self._n_sections = n_sections\n",
    "    self._axis = axis\n",
    "\n",
    "  def forward(self, inputs, weights):\n",
    "    del weights\n",
    "    x1, x2 = inputs\n",
    "\n",
    "    x1_split = np.split(x1, self._n_sections, self._axis)\n",
    "    x2_split = np.split(x2, self._n_sections, self._axis)\n",
    "\n",
    "    res = [np.concatenate(ys, -1) for ys in zip(x1_split, x2_split)]\n",
    "    return tuple(res)\n",
    "\n",
    "  def reverse(self, output, weights=(), state=(), new_state=(), **kwargs):\n",
    "    del weights, kwargs\n",
    "\n",
    "    x1_split = []\n",
    "    x2_split = []\n",
    "    for y in output:\n",
    "      y1, y2 = np.split(y, 2, -1)\n",
    "      x1_split.append(y1)\n",
    "      x2_split.append(y2)\n",
    "\n",
    "    x1 = np.concatenate(x1_split, self._axis)\n",
    "    x2 = np.concatenate(x2_split, self._axis)\n",
    "\n",
    "    return (x1, x2)\n",
    "\n",
    "  def reverse_and_grad(self, output, ct, weights=(), state=(), new_state=(),\n",
    "                       **kwargs):\n",
    "    del weights, kwargs\n",
    "    return self.reverse(output), (self.reverse(ct), ())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tl.layer()\n",
    "def Chunk(x, weights, n_sections=2, **kwargs):\n",
    "  del weights, kwargs\n",
    "  assert x.shape[1] % n_sections == 0\n",
    "  return np.reshape(x, (\n",
    "      x.shape[0] * n_sections,\n",
    "      x.shape[1] // n_sections,\n",
    "      ) + x.shape[2:])\n",
    "\n",
    "\n",
    "@tl.layer()\n",
    "def Unchunk(x, weights, n_sections=2, **kwargs):\n",
    "  del weights, kwargs\n",
    "  assert x.shape[0] % n_sections == 0\n",
    "  return np.reshape(x, (\n",
    "      x.shape[0] // n_sections,\n",
    "      x.shape[1] * n_sections,\n",
    "      ) + x.shape[2:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 894,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ReversibleHalfResidual(tl.ReversibleLayer, tl.Serial):\n",
    "  \"\"\"Half of a RevNet-style residual (only updates part of the hidden state).\"\"\"\n",
    "\n",
    "  def __init__(self, residual_layers):\n",
    "    self.compute_residual = tl.Serial(\n",
    "        # (x1_or_y1, x2) -> (x2, x1_or_y1, x2)\n",
    "        tl.Parallel([], tl.Dup()),\n",
    "        tl.Swap(),\n",
    "        tl.Parallel(residual_layers, [], []),\n",
    "    )\n",
    "\n",
    "    layers = [\n",
    "        self.compute_residual,\n",
    "        tl.Parallel(tl.Add(), [])\n",
    "    ]\n",
    "    super(ReversibleHalfResidual, self).__init__(layers)\n",
    "\n",
    "    self.subtract_top = tl.Parallel(tl.SubtractTop(), [])\n",
    "    self.reverse_layers = [self.compute_residual, self.subtract_top]\n",
    "\n",
    "  def reverse(self, output, weights=(), state=(), new_state=(), **kwargs):\n",
    "    reconstructed_x = output\n",
    "    rng = kwargs.pop('rng', None)\n",
    "    rngs = (None,) * self._n_layers\n",
    "    if rng is not None:\n",
    "      rngs = backend.random.split(rng, self._n_layers)\n",
    "    # Note that self.sublayers aligns exactly with self.reverse_layers in\n",
    "    # terms of parameter and rng usage, so no re-ordering is required.\n",
    "    for layer, p, s, ns, rng in zip(\n",
    "        self.reverse_layers, weights, state, new_state, rngs):\n",
    "      reconstructed_x = layer(reconstructed_x, weights=p,\n",
    "                              state=s, new_state=ns, rng=rng, **kwargs)\n",
    "    return reconstructed_x\n",
    "\n",
    "  def reverse_and_grad(self, output, ct, weights=(), state=(), new_state=(),\n",
    "                       **kwargs):\n",
    "    rng = kwargs.pop('rng', None)\n",
    "    rngs = (None,) * self._n_layers\n",
    "    if rng is not None:\n",
    "      rngs = backend.random.split(rng, self._n_layers)\n",
    "\n",
    "    def call_compute_residual(x, weights):\n",
    "      res = self.compute_residual(x, weights=weights, state=state[0],\n",
    "                                  rng=rngs[0], **kwargs)\n",
    "      return res\n",
    "\n",
    "    assert len(ct) == 2\n",
    "    ct = ((ct[0], ct[0], ct[1]))\n",
    "\n",
    "    stack_with_residual, vjpfun = jax.vjp(\n",
    "        call_compute_residual, output, weights[0])\n",
    "    reconstructed_x = self.subtract_top(\n",
    "        stack_with_residual, weights=weights[-1], state=state[-1], rng=rngs[-1],\n",
    "        **kwargs)\n",
    "\n",
    "    x_ct, residual_weights_ct = vjpfun(ct)\n",
    "    assert not jax.tree_util.tree_leaves(weights[-1])\n",
    "    add_top_weights_ct = weights[-1]\n",
    "    return reconstructed_x, (x_ct, [residual_weights_ct, add_top_weights_ct])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ApplyAttentionWrapper(tl.Parallel):\n",
    "  \"\"\"Like tl.Parallel(attention, [], []) but implements forward_and_backward.\"\"\"\n",
    "\n",
    "  def __init__(self, attention):\n",
    "    assert hasattr(attention, 'forward_and_backward')\n",
    "    super(ApplyAttentionWrapper, self).__init__(attention, [], [])\n",
    "    self.attention = attention\n",
    "\n",
    "  def forward_and_backward(self, inputs, ct, state, new_state, rng=None,\n",
    "                           **kwargs):\n",
    "    # Simultaneous forward pass and backprop through the attention mechanism.\n",
    "    qkv = inputs[:3]\n",
    "    passthrough = inputs[3:]\n",
    "    out_ct = ct[0]\n",
    "    passthrough_ct = ct[1:]\n",
    "    if rng is not None:\n",
    "      # Adjust RNG to match the forward pass.\n",
    "      rng = backend.random.split(rng, self._n_layers)[0]\n",
    "\n",
    "    out, qkv_ct = self.attention.forward_and_backward(\n",
    "        qkv, out_ct, rng=rng, state=state[0], new_state=new_state[0], **kwargs)\n",
    "    return (out,) + passthrough, qkv_ct + passthrough_ct\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ReversibleAttentionHalfResidual(tl.ReversibleLayer, tl.Serial):\n",
    "  \"\"\"Half of a RevNet-style residual that performs attention.\n",
    "  If inputs are (x1, x2), then outputs are (x1 + z, x2) where:\n",
    "  z = post_attention(attention(pre_attention(x1)))\n",
    "  Other than an efficiency optimization, this layer is equivalent to\n",
    "  ReversibleHalfResidual([pre_attention, attention, post_attention]).\n",
    "  The post_attention layers must be linear in their input (typically they will\n",
    "  consists of reshaping and dense linear layers), which allows the following\n",
    "  optimization. We can back-propagate the gradient signal from the output of\n",
    "  ReversibleAttentionHalfResidual to the output of the \"attention\" portion based\n",
    "  only on the network parameters. Then, attention.forward_and_backward can be\n",
    "  used to recover the output of the \"attention\" portion while simultaneously\n",
    "  performing the backward pass, which allows shared computation between the two\n",
    "  directions.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, pre_attention, attention, post_attention):\n",
    "    self.pre_attention = tl.Serial(\n",
    "        # (x1_or_y1, x2) -> (x2, x1_or_y1, x2)\n",
    "        tl.Parallel([], tl.Dup()),\n",
    "        tl.Swap(),\n",
    "        tl.Parallel(pre_attention, [], []),\n",
    "    )\n",
    "    assert hasattr(attention, 'forward_and_backward')\n",
    "    self.attention = ApplyAttentionWrapper(attention)\n",
    "    self.post_attention = tl.Parallel(post_attention, [], [])\n",
    "\n",
    "    layers = [\n",
    "        self.pre_attention,\n",
    "        self.attention,\n",
    "        self.post_attention,\n",
    "        tl.Parallel(tl.Add(), []),\n",
    "    ]\n",
    "    super(ReversibleAttentionHalfResidual, self).__init__(layers)\n",
    "\n",
    "    self.subtract_top = tl.Parallel(tl.SubtractTop(), [])\n",
    "    self.reverse_layers = [\n",
    "        self.pre_attention,\n",
    "        self.attention,\n",
    "        self.post_attention,\n",
    "        self.subtract_top,\n",
    "    ]\n",
    "\n",
    "  def reverse(self, output, weights=(), state=(), new_state=(), **kwargs):\n",
    "    rng = kwargs.pop('rng', None)\n",
    "    rngs = (None,) * self._n_layers\n",
    "    if rng is not None:\n",
    "      rngs = backend.random.split(rng, self._n_layers)\n",
    "\n",
    "    reconstructed_x = output\n",
    "    # Note that self.sublayers aligns exactly with self.reverse_layers in\n",
    "    # terms of parameter and rng usage, so no re-ordering is required.\n",
    "    for layer, p, s, ns, rng in zip(self.reverse_layers, weights,\n",
    "                                    state, new_state, rngs):\n",
    "      reconstructed_x = layer.reverse(reconstructed_x, weights=p,\n",
    "                                      state=s, new_state=ns, rng=rng, **kwargs)\n",
    "    return reconstructed_x\n",
    "\n",
    "  def reverse_and_grad(self, output, ct, weights=(), state=(), new_state=(),\n",
    "                       **kwargs):\n",
    "    rng = kwargs.pop('rng', None)\n",
    "    rngs = (None,) * self._n_layers\n",
    "    if rng is not None:\n",
    "      rngs = backend.random.split(rng, self._n_layers)\n",
    "\n",
    "    # Forward pass through self.pre_attention, while preparing for\n",
    "    # later backprop.\n",
    "    def call_pre_attention(x, weights):\n",
    "      res = self.pre_attention(x, weights=weights, state=state[0], rng=rngs[0],\n",
    "                               **kwargs)\n",
    "      return res\n",
    "    stack, pre_attention_vjpfun = jax.vjp(call_pre_attention,\n",
    "                                          output, weights[0])\n",
    "\n",
    "    # Backprop through adding the residual\n",
    "    assert len(ct) == 2\n",
    "    ct = saved_ct = (ct[0], ct[0], ct[1])\n",
    "\n",
    "    # Backprop through self.post_attention with respect to the inputs only\n",
    "    def call_post_attention(x):\n",
    "      res = self.post_attention(x, weights=weights[2], state=state[2],\n",
    "                                rng=rngs[2], **kwargs)\n",
    "      return res\n",
    "    # Note: these are *not* the actual inputs to self.post_attention.\n",
    "    # If self.post_attention is not linear, we will get incorrect gradients.\n",
    "    dummy_inputs = (stack[-3], stack[-2], stack[-1])\n",
    "    _, post_attention_vjpfun = jax.vjp(call_post_attention, dummy_inputs)\n",
    "    (ct,) = post_attention_vjpfun(ct)\n",
    "\n",
    "    # Simultaneous forward pass and backprop through the attention mechanism\n",
    "    stack, ct = self.attention.forward_and_backward(\n",
    "        stack, ct, rng=rngs[1], state=state[1], new_state=new_state[1],\n",
    "        **kwargs)\n",
    "    assert not jax.tree_util.tree_leaves(weights[1])\n",
    "    attention_weights_ct = weights[1]  # This is valid when weights is empty.\n",
    "\n",
    "    # Backprop through self.pre_attention\n",
    "    x_ct, pre_attention_weights_ct = pre_attention_vjpfun(ct)\n",
    "\n",
    "    # Forward pass for self.post_attention, and backprop with respect to the\n",
    "    # parameters only\n",
    "    def call_post_attention2(weights):\n",
    "      res = self.post_attention(stack, weights=weights, state=state[2],\n",
    "                                rng=rngs[2], **kwargs)\n",
    "      return res\n",
    "    stack, post_attention_vjpfun = jax.vjp(call_post_attention2, weights[2])\n",
    "    (post_attention_weights_ct,) = post_attention_vjpfun(saved_ct)\n",
    "\n",
    "    # Forward pass through subtracting the residual\n",
    "    reconstructed_x = self.subtract_top(\n",
    "        stack, weights=weights[-1], state=state[-1], rng=rngs[-1], **kwargs)\n",
    "\n",
    "    assert not jax.tree_util.tree_leaves(weights[-1])\n",
    "    add_top_weights_ct = weights[-1]\n",
    "    weights_ct = [\n",
    "        pre_attention_weights_ct,\n",
    "        attention_weights_ct,\n",
    "        post_attention_weights_ct,\n",
    "        add_top_weights_ct,\n",
    "    ]\n",
    "\n",
    "    return reconstructed_x, (x_ct, weights_ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def DecoderBlock(d_model, d_ff, d_attention_key, d_attention_value,\n",
    "                 n_heads, n_attention_chunks, attention_type,\n",
    "                 dropout, share_qk, ff_activation, ff_use_sru, mode):\n",
    "  \"\"\"Reversible transformer decoder layer.\n",
    "  Args:\n",
    "    d_model: int:  depth of embedding\n",
    "    d_ff: int: depth of feed-forward layer\n",
    "    d_attention_key: int: depth of key vector for each attention head\n",
    "    d_attention_value: int: depth of value vector for each attention head\n",
    "    n_heads: int: number of attention heads\n",
    "    n_attention_chunks: int: number of chunks for attention\n",
    "    attention_type: subclass of tl.BaseCausalAttention: attention class to use\n",
    "    dropout: float: dropout rate (how much to drop out)\n",
    "    share_qk: string, whether to share queries and keys\n",
    "    ff_activation: the non-linearity in feed-forward layer\n",
    "    ff_use_sru: int; if > 0, we use this many SRU layers instead of feed-forward\n",
    "    mode: str: 'train' or 'eval'\n",
    "  Returns:\n",
    "    the layer.\n",
    "  \"\"\"\n",
    "  if share_qk:\n",
    "    pre_attention = [\n",
    "        Chunk(n_sections=n_attention_chunks),  # pylint: disable=no-value-for-parameter\n",
    "        tl.LayerNorm(),\n",
    "        tl.Dup(),\n",
    "        tl.Parallel(\n",
    "            tl.ComputeAttentionHeads(n_heads=n_heads, d_head=d_attention_key),\n",
    "            tl.ComputeAttentionHeads(n_heads=n_heads, d_head=d_attention_value),\n",
    "        ),\n",
    "        tl.Dup(),\n",
    "    ]\n",
    "  else:\n",
    "    pre_attention = [\n",
    "        Chunk(n_sections=n_attention_chunks),  # pylint: disable=no-value-for-parameter\n",
    "        tl.LayerNorm(),\n",
    "        tl.Dup(), tl.Dup(),\n",
    "        tl.Parallel(\n",
    "            tl.ComputeAttentionHeads(n_heads=n_heads, d_head=d_attention_key),\n",
    "            tl.ComputeAttentionHeads(n_heads=n_heads, d_head=d_attention_key),\n",
    "            tl.ComputeAttentionHeads(n_heads=n_heads, d_head=d_attention_value),\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "  attention = attention_type(mode=mode)\n",
    "\n",
    "  # ReversibleAttentionHalfResidual requires that post_attention be linear in\n",
    "  # its input (so the backward pass can be computed without knowing the input)\n",
    "  post_attention = [\n",
    "      tl.ComputeAttentionOutput(n_heads=n_heads, d_model=d_model),\n",
    "      Unchunk(n_sections=n_attention_chunks),  # pylint: disable=no-value-for-parameter\n",
    "      BroadcastedDropout(rate=dropout, mode=mode),  # pylint: disable=no-value-for-parameter\n",
    "  ]\n",
    "\n",
    "  if ff_use_sru:\n",
    "    feed_forward = [tl.SRU(d_model) for _ in range(ff_use_sru)]\n",
    "  else:\n",
    "    feed_forward = [FeedForward(d_model, d_ff, dropout, ff_activation, mode)]\n",
    "\n",
    "  return [\n",
    "      ReversibleAttentionHalfResidual(pre_attention, attention, post_attention),\n",
    "      tl.ReversibleSwap(),\n",
    "      ReversibleHalfResidual(feed_forward),\n",
    "      tl.ReversibleSwap(),\n",
    "  ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ReformerLM(vocab_size,\n",
    "               d_model=512,\n",
    "               d_ff=2048,\n",
    "               d_attention_key=64,\n",
    "               d_attention_value=64,\n",
    "               n_layers=6,\n",
    "               n_heads=8,\n",
    "               dropout=0.1,\n",
    "               max_len=2048,\n",
    "               n_chunks=0,\n",
    "               n_attention_chunks=1,\n",
    "               attention_type=tl.DotProductCausalAttention,\n",
    "               share_qk=False,\n",
    "               axial_pos_shape=(),\n",
    "               d_axial_pos_embs=None,\n",
    "               ff_activation=tl.FastGelu,\n",
    "               ff_use_sru=0,\n",
    "               mode='train'):\n",
    "  \"\"\"Reversible transformer language model (only uses a decoder, no encoder).\n",
    "  Args:\n",
    "    vocab_size: int: vocab size\n",
    "    d_model: int:  depth of *each half* of the two-part features\n",
    "    d_ff: int: depth of feed-forward layer\n",
    "    d_attention_key: int: depth of key vector for each attention head\n",
    "    d_attention_value: int: depth of value vector for each attention head\n",
    "    n_layers: int: number of decoder layers\n",
    "    n_heads: int: number of attention heads\n",
    "    dropout: float: dropout rate (how much to drop out)\n",
    "    max_len: int: maximum symbol length for positional encoding\n",
    "    n_chunks: int: number of chunks (must match input pipeline)\n",
    "    n_attention_chunks: int: number of chunks for attention\n",
    "    attention_type: class: attention class to use, such as DotProductAttention.\n",
    "    share_qk: bool, whether to share queries and keys.\n",
    "    axial_pos_shape: tuple of ints: input shape to use for the axial position\n",
    "      encoding. If unset, axial position encoding is disabled.\n",
    "    d_axial_pos_embs: tuple of ints: depth of position embedding for each axis.\n",
    "      Tuple length must match axial_pos_shape, and values must sum to d_model.\n",
    "    ff_activation: the non-linearity in feed-forward layer\n",
    "    ff_use_sru: int; if > 0, we use this many SRU layers instead of feed-forward\n",
    "    mode: str: 'train', 'eval', or 'predict'\n",
    "  Returns:\n",
    "    the layer.\n",
    "  \"\"\"\n",
    "  if n_chunks == 0:\n",
    "    n_chunks = 1\n",
    "    concatenate_input_chunks = []\n",
    "  else:\n",
    "    concatenate_input_chunks = tl.Concatenate(n_items=n_chunks)\n",
    "\n",
    "  if not axial_pos_shape:\n",
    "    positional_encoding = tl.PositionalEncoding(\n",
    "        max_len=max_len, dropout=dropout, mode=mode)\n",
    "  else:\n",
    "    assert d_axial_pos_embs is not None\n",
    "    positional_encoding = tl.AxialPositionalEncoding(\n",
    "        shape=axial_pos_shape, d_embs=d_axial_pos_embs,\n",
    "        dropout_broadcast_dims=tuple(range(1, len(axial_pos_shape) + 1)),\n",
    "        dropout=dropout, mode=mode)\n",
    "\n",
    "  positional_embedder = [\n",
    "      tl.Embedding(d_model, vocab_size),\n",
    "      BroadcastedDropout(rate=dropout, mode=mode),  # pylint: disable=no-value-for-parameter\n",
    "      positional_encoding,\n",
    "  ]\n",
    "\n",
    "  decoder_blocks = []\n",
    "\n",
    "  if isinstance(attention_type, (tuple, list)):\n",
    "    assert n_layers % len(attention_type) == 0\n",
    "  else:\n",
    "    attention_type = [attention_type]\n",
    "  for layer_idx in range(n_layers):\n",
    "    layer_attention_type = attention_type[layer_idx % len(attention_type)]\n",
    "    decoder_block = DecoderBlock(\n",
    "        d_model, d_ff, d_attention_key, d_attention_value, n_heads,\n",
    "        n_attention_chunks,\n",
    "        attention_type=layer_attention_type,\n",
    "        dropout=dropout,\n",
    "        share_qk=(share_qk or issubclass(layer_attention_type,\n",
    "                                         tl.LSHCausalAttention)),\n",
    "        ff_activation=ff_activation,\n",
    "        ff_use_sru=ff_use_sru,\n",
    "        mode=mode)\n",
    "    decoder_blocks.append(decoder_block)\n",
    "\n",
    "  return tl.Serial(\n",
    "      concatenate_input_chunks,\n",
    "      tl.ShiftRight(mode=mode),\n",
    "      positional_embedder,\n",
    "      tl.Dup(),\n",
    "      tl.ReversibleSerial(decoder_blocks + [\n",
    "          SplitForOutput(n_sections=n_chunks, axis=-2),  # pylint: disable=no-value-for-parameter\n",
    "      ]),\n",
    "      Map([\n",
    "          # TODO(kitaev): Test whether dropout should go before or after the\n",
    "          # LayerNorm, and whether dropout broadcasting is needed here.\n",
    "          tl.LayerNorm(),\n",
    "          BroadcastedDropout(rate=dropout, mode=mode),  # pylint: disable=no-value-for-parameter\n",
    "          tl.Dense(vocab_size),\n",
    "          tl.LogSoftmax(),\n",
    "      ], n_sections=n_chunks),\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ReformerShortenLM(vocab_size,\n",
    "                      shorten_factor=1,\n",
    "                      d_embedding=256,\n",
    "                      d_model=512,\n",
    "                      d_ff=2048,\n",
    "                      d_attention_key=64,\n",
    "                      d_attention_value=64,\n",
    "                      n_layers=6,\n",
    "                      n_heads=8,\n",
    "                      dropout=0.1,\n",
    "                      max_len=2048,\n",
    "                      n_attention_chunks=1,\n",
    "                      attention_type=tl.DotProductCausalAttention,\n",
    "                      share_qk=False,\n",
    "                      axial_pos_shape=(),\n",
    "                      d_axial_pos_embs=None,\n",
    "                      ff_activation=tl.FastGelu,\n",
    "                      ff_use_sru=0,\n",
    "                      mode='train'):\n",
    "  \"\"\"Reversible transformer language model with shortening.\n",
    "  When shorten_factor is F and processing an input of shape [batch, length],\n",
    "  we embed the (shifted-right) input and then group each F elements (on length)\n",
    "  into a single vector -- so that in the end we process a tensor of shape\n",
    "    [batch, length // F, d_model]\n",
    "  almost until the end -- at the end it's un-shortend and a SRU is applied.\n",
    "  This reduces the length processed inside the main model body, effectively\n",
    "  making the model faster but possibly slightly less accurate.\n",
    "  Args:\n",
    "    vocab_size: int: vocab size\n",
    "    shorten_factor: by how much to shorten, see above\n",
    "    d_embedding: the depth of the embedding layer and final logits\n",
    "    d_model: int:  depth of *each half* of the two-part features\n",
    "    d_ff: int: depth of feed-forward layer\n",
    "    d_attention_key: int: depth of key vector for each attention head\n",
    "    d_attention_value: int: depth of value vector for each attention head\n",
    "    n_layers: int: number of decoder layers\n",
    "    n_heads: int: number of attention heads\n",
    "    dropout: float: dropout rate (how much to drop out)\n",
    "    max_len: int: maximum symbol length for positional encoding\n",
    "    n_attention_chunks: int: number of chunks for attention\n",
    "    attention_type: class: attention class to use, such as DotProductAttention.\n",
    "    share_qk: bool, whether to share queries and keys.\n",
    "    axial_pos_shape: tuple of ints: input shape to use for the axial position\n",
    "      encoding. If unset, axial position encoding is disabled.\n",
    "    d_axial_pos_embs: tuple of ints: depth of position embedding for each axis.\n",
    "      Tuple length must match axial_pos_shape, values must sum to d_embedding.\n",
    "    ff_activation: the non-linearity in feed-forward layer\n",
    "    ff_use_sru: int; if > 0, we use this many SRU layers instead of feed-forward\n",
    "    mode: str: 'train' or 'eval'\n",
    "  Returns:\n",
    "    the layer.\n",
    "  \"\"\"\n",
    "  assert mode != 'predict'  # TODO(lukaszkaiser,kitaev): fast inference\n",
    "\n",
    "  if not axial_pos_shape:\n",
    "    positional_encoding = tl.PositionalEncoding(\n",
    "        max_len=max_len, dropout=dropout, mode=mode)\n",
    "  else:\n",
    "    assert d_axial_pos_embs is not None\n",
    "    positional_encoding = tl.AxialPositionalEncoding(\n",
    "        shape=axial_pos_shape, d_embs=d_axial_pos_embs,\n",
    "        dropout_broadcast_dims=tuple(range(1, len(axial_pos_shape) + 1)),\n",
    "        dropout=dropout, mode=mode)\n",
    "\n",
    "  positional_embedder = [\n",
    "      tl.Embedding(d_embedding, vocab_size),\n",
    "      BroadcastedDropout(rate=dropout, mode=mode),  # pylint: disable=no-value-for-parameter\n",
    "      positional_encoding,\n",
    "  ]\n",
    "\n",
    "  decoder_blocks = []\n",
    "\n",
    "  if isinstance(attention_type, (tuple, list)):\n",
    "    assert n_layers % len(attention_type) == 0\n",
    "  else:\n",
    "    attention_type = [attention_type]\n",
    "  for layer_idx in range(n_layers):\n",
    "    layer_attention_type = attention_type[layer_idx % len(attention_type)]\n",
    "    decoder_block = DecoderBlock(\n",
    "        d_model, d_ff, d_attention_key, d_attention_value, n_heads,\n",
    "        n_attention_chunks,\n",
    "        attention_type=layer_attention_type,\n",
    "        dropout=dropout,\n",
    "        share_qk=(share_qk or issubclass(layer_attention_type,\n",
    "                                         tl.LSHCausalAttention)),\n",
    "        ff_activation=ff_activation,\n",
    "        ff_use_sru=ff_use_sru,\n",
    "        mode=mode)\n",
    "    decoder_blocks.append(decoder_block)\n",
    "\n",
    "  # pylint: disable=g-long-lambda\n",
    "  return tl.Serial(\n",
    "      tl.ShiftRight(),\n",
    "      positional_embedder,\n",
    "      tl.Dup(),              # Stack has (x, x), the first will be shortened\n",
    "      # Before shortening, we need to pad by shorten factor so as not to leak\n",
    "      # information into the future. To understand why, imagine shorten factor\n",
    "      # of 2 and sequence of length 4, so ABCD. If we shift just by 1, then we\n",
    "      # would have 0ABC, which gets grouped to [0A][BC] on input, which is\n",
    "      # predicting ABCD as targets. The problem is that [0A] has access to A\n",
    "      # and [BC] has access to C -- it will learn to copy it, peek into\n",
    "      # the future. Shifting twice to [00][AB] solves the problem as the first\n",
    "      # \"big\" symbol becomes all-0 and the rest is shifted enough.\n",
    "      tl.ShiftRight(n_shifts=shorten_factor - 1),\n",
    "      tl.Fn(lambda x: np.reshape(  # Shorten -- move to depth.\n",
    "          x, (x.shape[0], x.shape[1] // shorten_factor, -1)), n_out=1),\n",
    "      tl.Dense(d_model),\n",
    "      tl.Dup(),  # Stack has (short_x, short_x, x)\n",
    "      tl.ReversibleSerial(decoder_blocks),\n",
    "      tl.Select([0], n_in=2),\n",
    "      tl.LayerNorm(),\n",
    "      BroadcastedDropout(rate=dropout, mode=mode),  # pylint: disable=no-value-for-parameter\n",
    "      tl.Dense(shorten_factor * d_embedding),\n",
    "      tl.Fn(lambda x: np.reshape(  # Prolong back.\n",
    "          x, (x.shape[0], x.shape[1] * shorten_factor, -1)), n_out=1),\n",
    "      tl.Concatenate(),  # Concatenate with just the embeddings.\n",
    "      tl.CausalConv(d_embedding),\n",
    "      tl.Relu(),\n",
    "      tl.SRU(d_embedding),  # One RNN layer for conditional dependence.\n",
    "      tl.Dense(vocab_size),\n",
    "      tl.LogSoftmax()\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
