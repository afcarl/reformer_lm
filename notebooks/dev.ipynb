{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q -U trax\n",
    "! pip install -q tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import jax\n",
    "\n",
    "from trax import backend\n",
    "from trax import layers as tl\n",
    "from trax.backend import numpy as np\n",
    "from trax.layers.combinators import _pop_rng_and_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function _VariableFunctions.layer_norm>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.layer_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pMap(nn.Module):\n",
    "    \"\"\"Combinator for applying a layers to a list/tuple\"\"\"\n",
    "    def __init__(self, layer, n_sections=1, check_shapes=True):\n",
    "        \"\"\"\n",
    "        \n",
    "        :param layers: the layer you wish to apply to each element.\n",
    "        :param n_sections: number of sections to map to. defaults to 1.\n",
    "        :param check_shapes: tests to see the shapes are identical.\n",
    "        :returns: new layers with mapped layer to all elements.\n",
    "        \"\"\"\n",
    "        super(pMap, self).__init__()\n",
    "        if layer is None or isinstance(layer, (list, tuple)):\n",
    "            layer = nn.Sequential(*layer)\n",
    "        self._layer = layer\n",
    "        self._check_shapes = check_shapes\n",
    "        self._n_sections = n_sections\n",
    "        self.n_in = n_sections\n",
    "        self.n_out = n_sections\n",
    "        \n",
    "        \n",
    "    def forward_with_state(self, \n",
    "                           inputs, \n",
    "                           weights=tuple(), \n",
    "                           state=dict(), \n",
    "                           **kwargs):\n",
    "        if self._n_sections == 1:\n",
    "            if weights != tuple():\n",
    "                self._layer.weights = weights\n",
    "            if state != dict():\n",
    "                self._layer.load_state_dict(state)\n",
    "            results = self._layer(inputs, **kwargs)\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmap = pMap(layer=[\n",
    "    nn.LayerNorm(1,1), \n",
    "    nn.Linear(1,1),\n",
    "    nn.LogSoftmax()], n_sections=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pMap(\n",
       "  (_layer): Sequential(\n",
       "    (0): LayerNorm((1,), eps=1, elementwise_affine=True)\n",
       "    (1): Linear(in_features=1, out_features=1, bias=True)\n",
       "    (2): LogSoftmax()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = torch.tensor([0,1,2,3,4,5,6,7,8,9], dtype=torch.float)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'results' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-158-fdc74721bc3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpmap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_with_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-150-4b74cff78f25>\u001b[0m in \u001b[0;36mforward_with_state\u001b[0;34m(self, inputs, weights, state, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'results' referenced before assignment"
     ]
    }
   ],
   "source": [
    "pmap.forward_with_state(inputs=test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Map(tl.Layer):\n",
    "  \"\"\"Combinator for applying a layer to a list or tuple.\"\"\"\n",
    "\n",
    "  def __init__(self, layer, n_sections=1, check_shapes=True):\n",
    "    \"\"\"Initialize the combinator.\n",
    "    Args:\n",
    "      layer: a layer to apply to each element.\n",
    "      n_sections: how many sections to map to (default: 1).\n",
    "      check_shapes: whether to check that shapes are identical (default: true).\n",
    "    Returns:\n",
    "      A new layer representing mapping layer to all elements of the input.\n",
    "    \"\"\"\n",
    "    super(Map, self).__init__(n_in=n_sections, n_out=n_sections)\n",
    "    if layer is None or isinstance(layer, (list, tuple)):\n",
    "        layer = tl.Serial(layer)\n",
    "    self._layer = layer\n",
    "    \n",
    "    # Generally a Map should be applied to lists where all elements have\n",
    "    # the same shape -- because self._layer will only be initialized once\n",
    "    # and it could have different parameters for different shapes. But there\n",
    "    # are valid cases -- e.g., when self._layer has no parameters -- where we\n",
    "    # can apply Map to different shapes -- set check_shapes=False in such cases.\n",
    "    self._check_shapes = check_shapes\n",
    "    self._n_sections = n_sections\n",
    "\n",
    "    def forward_with_state(self, inputs, weights=(), state=(), **kwargs):\n",
    "      if self._n_sections == 1:\n",
    "        results = self._layer(inputs, weights=weights, state=state, **kwargs)\n",
    "      else:\n",
    "        rngs = _pop_rng_and_split(kwargs, len(inputs))\n",
    "        results = [self._layer(x, weights=weights, state=state, rng=r, **kwargs)\n",
    "                 for x, r in zip(inputs, rngs)]\n",
    "        results = tuple(results)\n",
    "      # TODO(kitaev): think about how to merge state across copies in the map.\n",
    "      return results, self._layer.state\n",
    "\n",
    "  def new_weights_and_state(self, input_signature):\n",
    "    if self._n_sections == 1:\n",
    "      return self._layer.init(input_signature)\n",
    "    first_shape = input_signature[0].shape\n",
    "    if self._check_shapes:\n",
    "      for shape_dtype in input_signature:\n",
    "        if shape_dtype.shape != first_shape:\n",
    "          raise ValueError('Map layer can only be applied to list of elements '\n",
    "                           'with the same shapes. This shape %s vs first shape '\n",
    "                           '%s.' % (str(shape_dtype.shape), str(first_shape)))\n",
    "    return self._layer.init(input_signature[0])\n",
    "\n",
    "  @tl.Layer.weights.setter\n",
    "  def weights(self, weights):\n",
    "    self._weights = self._layer.weights = weights\n",
    "\n",
    "  @tl.Layer.state.setter\n",
    "  def state(self, state):\n",
    "    self._state = self._layer.state = state\n",
    "\n",
    "  def _set_input_signature_recursive(self, input_signature):\n",
    "    self._input_signature = input_signature\n",
    "    self._layer._set_input_signature_recursive(input_signature)  # pylint: disable=protected-access\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BroadcastedDropout(tl.Layer):\n",
    "  \"\"\"Layer constructor function for a broadcasted dropout layer.\"\"\"\n",
    "\n",
    "  def __init__(self, rate=0.0, mode='train', broadcast_dims=(-2,)):\n",
    "    super(BroadcastedDropout, self).__init__()\n",
    "    self._rate = rate\n",
    "    if self._rate >= 1.0:\n",
    "      raise ValueError('Dropout rate (%f) must be lower than 1.' % rate)\n",
    "    self._broadcast_dims = broadcast_dims\n",
    "    self._mode = mode\n",
    "\n",
    "  def forward_with_state(self, x, weights, state, rng):\n",
    "    \"\"\"Dropout, with broadcasting to save memory.\"\"\"\n",
    "    del weights\n",
    "    if rng is None:\n",
    "      raise ValueError('BroadcastedDropout requires rng kwarg.')\n",
    "    if self._mode == 'train' and self._rate > 0.0:\n",
    "      noise_shape = list(x.shape)\n",
    "      for dim in self._broadcast_dims:\n",
    "        noise_shape[dim] = 1\n",
    "      keep_prob = jax.lax.tie_in(rng, 1.0 - self._rate)\n",
    "      keep = backend.random.bernoulli(rng, keep_prob, tuple(noise_shape))\n",
    "      multiplier = keep.astype(x.dtype) / jax.lax.tie_in(keep, keep_prob)\n",
    "      return x * multiplier, state\n",
    "    else:\n",
    "      return x, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FeedForward(d_model, d_ff, dropout, activation, mode):\n",
    "  \"\"\"Feed-forward block with layer normalization at start.\"\"\"\n",
    "  return [\n",
    "      tl.LayerNorm(),\n",
    "      tl.Dense(d_ff),\n",
    "      BroadcastedDropout(rate=dropout, mode=mode),  # pylint: disable=no-value-for-parameter\n",
    "      activation(),\n",
    "      tl.Dense(d_model),\n",
    "      BroadcastedDropout(rate=dropout, mode=mode),  # pylint: disable=no-value-for-parameter\n",
    "  ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitForOutput(tl.ReversibleLayer):\n",
    "  \"\"\"Splits activations into sections (for use right before the output layer).\n",
    "  After the reversible portion of the network, there is a final output portion\n",
    "  that's non-reversible (which at minimum includes normalization, output\n",
    "  projection, and log-softmax). The output portion needs to operate on chunks\n",
    "  of the sequence to avoid running out of memory for large vocabulary sizes.\n",
    "  This layer concatenates the two subparts of the activations along the feature\n",
    "  dimension, and then splits into chunks along the time dimension. We implement\n",
    "  it is a subclass of tl.ReversibleLayer because we want to ensure that multiple\n",
    "  copies of the activations don't exist simultaneously except in the middle of a\n",
    "  memory copy operation.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, n_sections=2, axis=-2):\n",
    "    super(SplitForOutput, self).__init__(n_in=2, n_out=n_sections)\n",
    "    self._n_sections = n_sections\n",
    "    self._axis = axis\n",
    "\n",
    "  def forward(self, inputs, weights):\n",
    "    del weights\n",
    "    x1, x2 = inputs\n",
    "\n",
    "    x1_split = np.split(x1, self._n_sections, self._axis)\n",
    "    x2_split = np.split(x2, self._n_sections, self._axis)\n",
    "\n",
    "    res = [np.concatenate(ys, -1) for ys in zip(x1_split, x2_split)]\n",
    "    return tuple(res)\n",
    "\n",
    "  def reverse(self, output, weights=(), state=(), new_state=(), **kwargs):\n",
    "    del weights, kwargs\n",
    "\n",
    "    x1_split = []\n",
    "    x2_split = []\n",
    "    for y in output:\n",
    "      y1, y2 = np.split(y, 2, -1)\n",
    "      x1_split.append(y1)\n",
    "      x2_split.append(y2)\n",
    "\n",
    "    x1 = np.concatenate(x1_split, self._axis)\n",
    "    x2 = np.concatenate(x2_split, self._axis)\n",
    "\n",
    "    return (x1, x2)\n",
    "\n",
    "  def reverse_and_grad(self, output, ct, weights=(), state=(), new_state=(),\n",
    "                       **kwargs):\n",
    "    del weights, kwargs\n",
    "    return self.reverse(output), (self.reverse(ct), ())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tl.layer()\n",
    "def Chunk(x, weights, n_sections=2, **kwargs):\n",
    "  del weights, kwargs\n",
    "  assert x.shape[1] % n_sections == 0\n",
    "  return np.reshape(x, (\n",
    "      x.shape[0] * n_sections,\n",
    "      x.shape[1] // n_sections,\n",
    "      ) + x.shape[2:])\n",
    "\n",
    "\n",
    "@tl.layer()\n",
    "def Unchunk(x, weights, n_sections=2, **kwargs):\n",
    "  del weights, kwargs\n",
    "  assert x.shape[0] % n_sections == 0\n",
    "  return np.reshape(x, (\n",
    "      x.shape[0] // n_sections,\n",
    "      x.shape[1] * n_sections,\n",
    "      ) + x.shape[2:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ReversibleHalfResidual(tl.ReversibleLayer, tl.Serial):\n",
    "  \"\"\"Half of a RevNet-style residual (only updates part of the hidden state).\"\"\"\n",
    "\n",
    "  def __init__(self, residual_layers):\n",
    "    self.compute_residual = tl.Serial(\n",
    "        # (x1_or_y1, x2) -> (x2, x1_or_y1, x2)\n",
    "        tl.Parallel([], tl.Dup()),\n",
    "        tl.Swap(),\n",
    "        tl.Parallel(residual_layers, [], []),\n",
    "    )\n",
    "\n",
    "    layers = [\n",
    "        self.compute_residual,\n",
    "        tl.Parallel(tl.Add(), [])\n",
    "    ]\n",
    "    super(ReversibleHalfResidual, self).__init__(layers)\n",
    "\n",
    "    self.subtract_top = tl.Parallel(tl.SubtractTop(), [])\n",
    "    self.reverse_layers = [self.compute_residual, self.subtract_top]\n",
    "\n",
    "  def reverse(self, output, weights=(), state=(), new_state=(), **kwargs):\n",
    "    reconstructed_x = output\n",
    "    rng = kwargs.pop('rng', None)\n",
    "    rngs = (None,) * self._n_layers\n",
    "    if rng is not None:\n",
    "      rngs = backend.random.split(rng, self._n_layers)\n",
    "    # Note that self.sublayers aligns exactly with self.reverse_layers in\n",
    "    # terms of parameter and rng usage, so no re-ordering is required.\n",
    "    for layer, p, s, ns, rng in zip(\n",
    "        self.reverse_layers, weights, state, new_state, rngs):\n",
    "      reconstructed_x = layer(reconstructed_x, weights=p,\n",
    "                              state=s, new_state=ns, rng=rng, **kwargs)\n",
    "    return reconstructed_x\n",
    "\n",
    "  def reverse_and_grad(self, output, ct, weights=(), state=(), new_state=(),\n",
    "                       **kwargs):\n",
    "    rng = kwargs.pop('rng', None)\n",
    "    rngs = (None,) * self._n_layers\n",
    "    if rng is not None:\n",
    "      rngs = backend.random.split(rng, self._n_layers)\n",
    "\n",
    "    def call_compute_residual(x, weights):\n",
    "      res = self.compute_residual(x, weights=weights, state=state[0],\n",
    "                                  rng=rngs[0], **kwargs)\n",
    "      return res\n",
    "\n",
    "    assert len(ct) == 2\n",
    "    ct = ((ct[0], ct[0], ct[1]))\n",
    "\n",
    "    stack_with_residual, vjpfun = jax.vjp(\n",
    "        call_compute_residual, output, weights[0])\n",
    "    reconstructed_x = self.subtract_top(\n",
    "        stack_with_residual, weights=weights[-1], state=state[-1], rng=rngs[-1],\n",
    "        **kwargs)\n",
    "\n",
    "    x_ct, residual_weights_ct = vjpfun(ct)\n",
    "    assert not jax.tree_util.tree_leaves(weights[-1])\n",
    "    add_top_weights_ct = weights[-1]\n",
    "    return reconstructed_x, (x_ct, [residual_weights_ct, add_top_weights_ct])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ApplyAttentionWrapper(tl.Parallel):\n",
    "  \"\"\"Like tl.Parallel(attention, [], []) but implements forward_and_backward.\"\"\"\n",
    "\n",
    "  def __init__(self, attention):\n",
    "    assert hasattr(attention, 'forward_and_backward')\n",
    "    super(ApplyAttentionWrapper, self).__init__(attention, [], [])\n",
    "    self.attention = attention\n",
    "\n",
    "  def forward_and_backward(self, inputs, ct, state, new_state, rng=None,\n",
    "                           **kwargs):\n",
    "    # Simultaneous forward pass and backprop through the attention mechanism.\n",
    "    qkv = inputs[:3]\n",
    "    passthrough = inputs[3:]\n",
    "    out_ct = ct[0]\n",
    "    passthrough_ct = ct[1:]\n",
    "    if rng is not None:\n",
    "      # Adjust RNG to match the forward pass.\n",
    "      rng = backend.random.split(rng, self._n_layers)[0]\n",
    "\n",
    "    out, qkv_ct = self.attention.forward_and_backward(\n",
    "        qkv, out_ct, rng=rng, state=state[0], new_state=new_state[0], **kwargs)\n",
    "    return (out,) + passthrough, qkv_ct + passthrough_ct\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ReversibleAttentionHalfResidual(tl.ReversibleLayer, tl.Serial):\n",
    "  \"\"\"Half of a RevNet-style residual that performs attention.\n",
    "  If inputs are (x1, x2), then outputs are (x1 + z, x2) where:\n",
    "  z = post_attention(attention(pre_attention(x1)))\n",
    "  Other than an efficiency optimization, this layer is equivalent to\n",
    "  ReversibleHalfResidual([pre_attention, attention, post_attention]).\n",
    "  The post_attention layers must be linear in their input (typically they will\n",
    "  consists of reshaping and dense linear layers), which allows the following\n",
    "  optimization. We can back-propagate the gradient signal from the output of\n",
    "  ReversibleAttentionHalfResidual to the output of the \"attention\" portion based\n",
    "  only on the network parameters. Then, attention.forward_and_backward can be\n",
    "  used to recover the output of the \"attention\" portion while simultaneously\n",
    "  performing the backward pass, which allows shared computation between the two\n",
    "  directions.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, pre_attention, attention, post_attention):\n",
    "    self.pre_attention = tl.Serial(\n",
    "        # (x1_or_y1, x2) -> (x2, x1_or_y1, x2)\n",
    "        tl.Parallel([], tl.Dup()),\n",
    "        tl.Swap(),\n",
    "        tl.Parallel(pre_attention, [], []),\n",
    "    )\n",
    "    assert hasattr(attention, 'forward_and_backward')\n",
    "    self.attention = ApplyAttentionWrapper(attention)\n",
    "    self.post_attention = tl.Parallel(post_attention, [], [])\n",
    "\n",
    "    layers = [\n",
    "        self.pre_attention,\n",
    "        self.attention,\n",
    "        self.post_attention,\n",
    "        tl.Parallel(tl.Add(), []),\n",
    "    ]\n",
    "    super(ReversibleAttentionHalfResidual, self).__init__(layers)\n",
    "\n",
    "    self.subtract_top = tl.Parallel(tl.SubtractTop(), [])\n",
    "    self.reverse_layers = [\n",
    "        self.pre_attention,\n",
    "        self.attention,\n",
    "        self.post_attention,\n",
    "        self.subtract_top,\n",
    "    ]\n",
    "\n",
    "  def reverse(self, output, weights=(), state=(), new_state=(), **kwargs):\n",
    "    rng = kwargs.pop('rng', None)\n",
    "    rngs = (None,) * self._n_layers\n",
    "    if rng is not None:\n",
    "      rngs = backend.random.split(rng, self._n_layers)\n",
    "\n",
    "    reconstructed_x = output\n",
    "    # Note that self.sublayers aligns exactly with self.reverse_layers in\n",
    "    # terms of parameter and rng usage, so no re-ordering is required.\n",
    "    for layer, p, s, ns, rng in zip(self.reverse_layers, weights,\n",
    "                                    state, new_state, rngs):\n",
    "      reconstructed_x = layer.reverse(reconstructed_x, weights=p,\n",
    "                                      state=s, new_state=ns, rng=rng, **kwargs)\n",
    "    return reconstructed_x\n",
    "\n",
    "  def reverse_and_grad(self, output, ct, weights=(), state=(), new_state=(),\n",
    "                       **kwargs):\n",
    "    rng = kwargs.pop('rng', None)\n",
    "    rngs = (None,) * self._n_layers\n",
    "    if rng is not None:\n",
    "      rngs = backend.random.split(rng, self._n_layers)\n",
    "\n",
    "    # Forward pass through self.pre_attention, while preparing for\n",
    "    # later backprop.\n",
    "    def call_pre_attention(x, weights):\n",
    "      res = self.pre_attention(x, weights=weights, state=state[0], rng=rngs[0],\n",
    "                               **kwargs)\n",
    "      return res\n",
    "    stack, pre_attention_vjpfun = jax.vjp(call_pre_attention,\n",
    "                                          output, weights[0])\n",
    "\n",
    "    # Backprop through adding the residual\n",
    "    assert len(ct) == 2\n",
    "    ct = saved_ct = (ct[0], ct[0], ct[1])\n",
    "\n",
    "    # Backprop through self.post_attention with respect to the inputs only\n",
    "    def call_post_attention(x):\n",
    "      res = self.post_attention(x, weights=weights[2], state=state[2],\n",
    "                                rng=rngs[2], **kwargs)\n",
    "      return res\n",
    "    # Note: these are *not* the actual inputs to self.post_attention.\n",
    "    # If self.post_attention is not linear, we will get incorrect gradients.\n",
    "    dummy_inputs = (stack[-3], stack[-2], stack[-1])\n",
    "    _, post_attention_vjpfun = jax.vjp(call_post_attention, dummy_inputs)\n",
    "    (ct,) = post_attention_vjpfun(ct)\n",
    "\n",
    "    # Simultaneous forward pass and backprop through the attention mechanism\n",
    "    stack, ct = self.attention.forward_and_backward(\n",
    "        stack, ct, rng=rngs[1], state=state[1], new_state=new_state[1],\n",
    "        **kwargs)\n",
    "    assert not jax.tree_util.tree_leaves(weights[1])\n",
    "    attention_weights_ct = weights[1]  # This is valid when weights is empty.\n",
    "\n",
    "    # Backprop through self.pre_attention\n",
    "    x_ct, pre_attention_weights_ct = pre_attention_vjpfun(ct)\n",
    "\n",
    "    # Forward pass for self.post_attention, and backprop with respect to the\n",
    "    # parameters only\n",
    "    def call_post_attention2(weights):\n",
    "      res = self.post_attention(stack, weights=weights, state=state[2],\n",
    "                                rng=rngs[2], **kwargs)\n",
    "      return res\n",
    "    stack, post_attention_vjpfun = jax.vjp(call_post_attention2, weights[2])\n",
    "    (post_attention_weights_ct,) = post_attention_vjpfun(saved_ct)\n",
    "\n",
    "    # Forward pass through subtracting the residual\n",
    "    reconstructed_x = self.subtract_top(\n",
    "        stack, weights=weights[-1], state=state[-1], rng=rngs[-1], **kwargs)\n",
    "\n",
    "    assert not jax.tree_util.tree_leaves(weights[-1])\n",
    "    add_top_weights_ct = weights[-1]\n",
    "    weights_ct = [\n",
    "        pre_attention_weights_ct,\n",
    "        attention_weights_ct,\n",
    "        post_attention_weights_ct,\n",
    "        add_top_weights_ct,\n",
    "    ]\n",
    "\n",
    "    return reconstructed_x, (x_ct, weights_ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def DecoderBlock(d_model, d_ff, d_attention_key, d_attention_value,\n",
    "                 n_heads, n_attention_chunks, attention_type,\n",
    "                 dropout, share_qk, ff_activation, ff_use_sru, mode):\n",
    "  \"\"\"Reversible transformer decoder layer.\n",
    "  Args:\n",
    "    d_model: int:  depth of embedding\n",
    "    d_ff: int: depth of feed-forward layer\n",
    "    d_attention_key: int: depth of key vector for each attention head\n",
    "    d_attention_value: int: depth of value vector for each attention head\n",
    "    n_heads: int: number of attention heads\n",
    "    n_attention_chunks: int: number of chunks for attention\n",
    "    attention_type: subclass of tl.BaseCausalAttention: attention class to use\n",
    "    dropout: float: dropout rate (how much to drop out)\n",
    "    share_qk: string, whether to share queries and keys\n",
    "    ff_activation: the non-linearity in feed-forward layer\n",
    "    ff_use_sru: int; if > 0, we use this many SRU layers instead of feed-forward\n",
    "    mode: str: 'train' or 'eval'\n",
    "  Returns:\n",
    "    the layer.\n",
    "  \"\"\"\n",
    "  if share_qk:\n",
    "    pre_attention = [\n",
    "        Chunk(n_sections=n_attention_chunks),  # pylint: disable=no-value-for-parameter\n",
    "        tl.LayerNorm(),\n",
    "        tl.Dup(),\n",
    "        tl.Parallel(\n",
    "            tl.ComputeAttentionHeads(n_heads=n_heads, d_head=d_attention_key),\n",
    "            tl.ComputeAttentionHeads(n_heads=n_heads, d_head=d_attention_value),\n",
    "        ),\n",
    "        tl.Dup(),\n",
    "    ]\n",
    "  else:\n",
    "    pre_attention = [\n",
    "        Chunk(n_sections=n_attention_chunks),  # pylint: disable=no-value-for-parameter\n",
    "        tl.LayerNorm(),\n",
    "        tl.Dup(), tl.Dup(),\n",
    "        tl.Parallel(\n",
    "            tl.ComputeAttentionHeads(n_heads=n_heads, d_head=d_attention_key),\n",
    "            tl.ComputeAttentionHeads(n_heads=n_heads, d_head=d_attention_key),\n",
    "            tl.ComputeAttentionHeads(n_heads=n_heads, d_head=d_attention_value),\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "  attention = attention_type(mode=mode)\n",
    "\n",
    "  # ReversibleAttentionHalfResidual requires that post_attention be linear in\n",
    "  # its input (so the backward pass can be computed without knowing the input)\n",
    "  post_attention = [\n",
    "      tl.ComputeAttentionOutput(n_heads=n_heads, d_model=d_model),\n",
    "      Unchunk(n_sections=n_attention_chunks),  # pylint: disable=no-value-for-parameter\n",
    "      BroadcastedDropout(rate=dropout, mode=mode),  # pylint: disable=no-value-for-parameter\n",
    "  ]\n",
    "\n",
    "  if ff_use_sru:\n",
    "    feed_forward = [tl.SRU(d_model) for _ in range(ff_use_sru)]\n",
    "  else:\n",
    "    feed_forward = [FeedForward(d_model, d_ff, dropout, ff_activation, mode)]\n",
    "\n",
    "  return [\n",
    "      ReversibleAttentionHalfResidual(pre_attention, attention, post_attention),\n",
    "      tl.ReversibleSwap(),\n",
    "      ReversibleHalfResidual(feed_forward),\n",
    "      tl.ReversibleSwap(),\n",
    "  ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ReformerLM(vocab_size,\n",
    "               d_model=512,\n",
    "               d_ff=2048,\n",
    "               d_attention_key=64,\n",
    "               d_attention_value=64,\n",
    "               n_layers=6,\n",
    "               n_heads=8,\n",
    "               dropout=0.1,\n",
    "               max_len=2048,\n",
    "               n_chunks=0,\n",
    "               n_attention_chunks=1,\n",
    "               attention_type=tl.DotProductCausalAttention,\n",
    "               share_qk=False,\n",
    "               axial_pos_shape=(),\n",
    "               d_axial_pos_embs=None,\n",
    "               ff_activation=tl.FastGelu,\n",
    "               ff_use_sru=0,\n",
    "               mode='train'):\n",
    "  \"\"\"Reversible transformer language model (only uses a decoder, no encoder).\n",
    "  Args:\n",
    "    vocab_size: int: vocab size\n",
    "    d_model: int:  depth of *each half* of the two-part features\n",
    "    d_ff: int: depth of feed-forward layer\n",
    "    d_attention_key: int: depth of key vector for each attention head\n",
    "    d_attention_value: int: depth of value vector for each attention head\n",
    "    n_layers: int: number of decoder layers\n",
    "    n_heads: int: number of attention heads\n",
    "    dropout: float: dropout rate (how much to drop out)\n",
    "    max_len: int: maximum symbol length for positional encoding\n",
    "    n_chunks: int: number of chunks (must match input pipeline)\n",
    "    n_attention_chunks: int: number of chunks for attention\n",
    "    attention_type: class: attention class to use, such as DotProductAttention.\n",
    "    share_qk: bool, whether to share queries and keys.\n",
    "    axial_pos_shape: tuple of ints: input shape to use for the axial position\n",
    "      encoding. If unset, axial position encoding is disabled.\n",
    "    d_axial_pos_embs: tuple of ints: depth of position embedding for each axis.\n",
    "      Tuple length must match axial_pos_shape, and values must sum to d_model.\n",
    "    ff_activation: the non-linearity in feed-forward layer\n",
    "    ff_use_sru: int; if > 0, we use this many SRU layers instead of feed-forward\n",
    "    mode: str: 'train', 'eval', or 'predict'\n",
    "  Returns:\n",
    "    the layer.\n",
    "  \"\"\"\n",
    "  if n_chunks == 0:\n",
    "    n_chunks = 1\n",
    "    concatenate_input_chunks = []\n",
    "  else:\n",
    "    concatenate_input_chunks = tl.Concatenate(n_items=n_chunks)\n",
    "\n",
    "  if not axial_pos_shape:\n",
    "    positional_encoding = tl.PositionalEncoding(\n",
    "        max_len=max_len, dropout=dropout, mode=mode)\n",
    "  else:\n",
    "    assert d_axial_pos_embs is not None\n",
    "    positional_encoding = tl.AxialPositionalEncoding(\n",
    "        shape=axial_pos_shape, d_embs=d_axial_pos_embs,\n",
    "        dropout_broadcast_dims=tuple(range(1, len(axial_pos_shape) + 1)),\n",
    "        dropout=dropout, mode=mode)\n",
    "\n",
    "  positional_embedder = [\n",
    "      tl.Embedding(d_model, vocab_size),\n",
    "      BroadcastedDropout(rate=dropout, mode=mode),  # pylint: disable=no-value-for-parameter\n",
    "      positional_encoding,\n",
    "  ]\n",
    "\n",
    "  decoder_blocks = []\n",
    "\n",
    "  if isinstance(attention_type, (tuple, list)):\n",
    "    assert n_layers % len(attention_type) == 0\n",
    "  else:\n",
    "    attention_type = [attention_type]\n",
    "  for layer_idx in range(n_layers):\n",
    "    layer_attention_type = attention_type[layer_idx % len(attention_type)]\n",
    "    decoder_block = DecoderBlock(\n",
    "        d_model, d_ff, d_attention_key, d_attention_value, n_heads,\n",
    "        n_attention_chunks,\n",
    "        attention_type=layer_attention_type,\n",
    "        dropout=dropout,\n",
    "        share_qk=(share_qk or issubclass(layer_attention_type,\n",
    "                                         tl.LSHCausalAttention)),\n",
    "        ff_activation=ff_activation,\n",
    "        ff_use_sru=ff_use_sru,\n",
    "        mode=mode)\n",
    "    decoder_blocks.append(decoder_block)\n",
    "\n",
    "  return tl.Serial(\n",
    "      concatenate_input_chunks,\n",
    "      tl.ShiftRight(mode=mode),\n",
    "      positional_embedder,\n",
    "      tl.Dup(),\n",
    "      tl.ReversibleSerial(decoder_blocks + [\n",
    "          SplitForOutput(n_sections=n_chunks, axis=-2),  # pylint: disable=no-value-for-parameter\n",
    "      ]),\n",
    "      Map([\n",
    "          # TODO(kitaev): Test whether dropout should go before or after the\n",
    "          # LayerNorm, and whether dropout broadcasting is needed here.\n",
    "          tl.LayerNorm(),\n",
    "          BroadcastedDropout(rate=dropout, mode=mode),  # pylint: disable=no-value-for-parameter\n",
    "          tl.Dense(vocab_size),\n",
    "          tl.LogSoftmax(),\n",
    "      ], n_sections=n_chunks),\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ReformerShortenLM(vocab_size,\n",
    "                      shorten_factor=1,\n",
    "                      d_embedding=256,\n",
    "                      d_model=512,\n",
    "                      d_ff=2048,\n",
    "                      d_attention_key=64,\n",
    "                      d_attention_value=64,\n",
    "                      n_layers=6,\n",
    "                      n_heads=8,\n",
    "                      dropout=0.1,\n",
    "                      max_len=2048,\n",
    "                      n_attention_chunks=1,\n",
    "                      attention_type=tl.DotProductCausalAttention,\n",
    "                      share_qk=False,\n",
    "                      axial_pos_shape=(),\n",
    "                      d_axial_pos_embs=None,\n",
    "                      ff_activation=tl.FastGelu,\n",
    "                      ff_use_sru=0,\n",
    "                      mode='train'):\n",
    "  \"\"\"Reversible transformer language model with shortening.\n",
    "  When shorten_factor is F and processing an input of shape [batch, length],\n",
    "  we embed the (shifted-right) input and then group each F elements (on length)\n",
    "  into a single vector -- so that in the end we process a tensor of shape\n",
    "    [batch, length // F, d_model]\n",
    "  almost until the end -- at the end it's un-shortend and a SRU is applied.\n",
    "  This reduces the length processed inside the main model body, effectively\n",
    "  making the model faster but possibly slightly less accurate.\n",
    "  Args:\n",
    "    vocab_size: int: vocab size\n",
    "    shorten_factor: by how much to shorten, see above\n",
    "    d_embedding: the depth of the embedding layer and final logits\n",
    "    d_model: int:  depth of *each half* of the two-part features\n",
    "    d_ff: int: depth of feed-forward layer\n",
    "    d_attention_key: int: depth of key vector for each attention head\n",
    "    d_attention_value: int: depth of value vector for each attention head\n",
    "    n_layers: int: number of decoder layers\n",
    "    n_heads: int: number of attention heads\n",
    "    dropout: float: dropout rate (how much to drop out)\n",
    "    max_len: int: maximum symbol length for positional encoding\n",
    "    n_attention_chunks: int: number of chunks for attention\n",
    "    attention_type: class: attention class to use, such as DotProductAttention.\n",
    "    share_qk: bool, whether to share queries and keys.\n",
    "    axial_pos_shape: tuple of ints: input shape to use for the axial position\n",
    "      encoding. If unset, axial position encoding is disabled.\n",
    "    d_axial_pos_embs: tuple of ints: depth of position embedding for each axis.\n",
    "      Tuple length must match axial_pos_shape, values must sum to d_embedding.\n",
    "    ff_activation: the non-linearity in feed-forward layer\n",
    "    ff_use_sru: int; if > 0, we use this many SRU layers instead of feed-forward\n",
    "    mode: str: 'train' or 'eval'\n",
    "  Returns:\n",
    "    the layer.\n",
    "  \"\"\"\n",
    "  assert mode != 'predict'  # TODO(lukaszkaiser,kitaev): fast inference\n",
    "\n",
    "  if not axial_pos_shape:\n",
    "    positional_encoding = tl.PositionalEncoding(\n",
    "        max_len=max_len, dropout=dropout, mode=mode)\n",
    "  else:\n",
    "    assert d_axial_pos_embs is not None\n",
    "    positional_encoding = tl.AxialPositionalEncoding(\n",
    "        shape=axial_pos_shape, d_embs=d_axial_pos_embs,\n",
    "        dropout_broadcast_dims=tuple(range(1, len(axial_pos_shape) + 1)),\n",
    "        dropout=dropout, mode=mode)\n",
    "\n",
    "  positional_embedder = [\n",
    "      tl.Embedding(d_embedding, vocab_size),\n",
    "      BroadcastedDropout(rate=dropout, mode=mode),  # pylint: disable=no-value-for-parameter\n",
    "      positional_encoding,\n",
    "  ]\n",
    "\n",
    "  decoder_blocks = []\n",
    "\n",
    "  if isinstance(attention_type, (tuple, list)):\n",
    "    assert n_layers % len(attention_type) == 0\n",
    "  else:\n",
    "    attention_type = [attention_type]\n",
    "  for layer_idx in range(n_layers):\n",
    "    layer_attention_type = attention_type[layer_idx % len(attention_type)]\n",
    "    decoder_block = DecoderBlock(\n",
    "        d_model, d_ff, d_attention_key, d_attention_value, n_heads,\n",
    "        n_attention_chunks,\n",
    "        attention_type=layer_attention_type,\n",
    "        dropout=dropout,\n",
    "        share_qk=(share_qk or issubclass(layer_attention_type,\n",
    "                                         tl.LSHCausalAttention)),\n",
    "        ff_activation=ff_activation,\n",
    "        ff_use_sru=ff_use_sru,\n",
    "        mode=mode)\n",
    "    decoder_blocks.append(decoder_block)\n",
    "\n",
    "  # pylint: disable=g-long-lambda\n",
    "  return tl.Serial(\n",
    "      tl.ShiftRight(),\n",
    "      positional_embedder,\n",
    "      tl.Dup(),              # Stack has (x, x), the first will be shortened\n",
    "      # Before shortening, we need to pad by shorten factor so as not to leak\n",
    "      # information into the future. To understand why, imagine shorten factor\n",
    "      # of 2 and sequence of length 4, so ABCD. If we shift just by 1, then we\n",
    "      # would have 0ABC, which gets grouped to [0A][BC] on input, which is\n",
    "      # predicting ABCD as targets. The problem is that [0A] has access to A\n",
    "      # and [BC] has access to C -- it will learn to copy it, peek into\n",
    "      # the future. Shifting twice to [00][AB] solves the problem as the first\n",
    "      # \"big\" symbol becomes all-0 and the rest is shifted enough.\n",
    "      tl.ShiftRight(n_shifts=shorten_factor - 1),\n",
    "      tl.Fn(lambda x: np.reshape(  # Shorten -- move to depth.\n",
    "          x, (x.shape[0], x.shape[1] // shorten_factor, -1)), n_out=1),\n",
    "      tl.Dense(d_model),\n",
    "      tl.Dup(),  # Stack has (short_x, short_x, x)\n",
    "      tl.ReversibleSerial(decoder_blocks),\n",
    "      tl.Select([0], n_in=2),\n",
    "      tl.LayerNorm(),\n",
    "      BroadcastedDropout(rate=dropout, mode=mode),  # pylint: disable=no-value-for-parameter\n",
    "      tl.Dense(shorten_factor * d_embedding),\n",
    "      tl.Fn(lambda x: np.reshape(  # Prolong back.\n",
    "          x, (x.shape[0], x.shape[1] * shorten_factor, -1)), n_out=1),\n",
    "      tl.Concatenate(),  # Concatenate with just the embeddings.\n",
    "      tl.CausalConv(d_embedding),\n",
    "      tl.Relu(),\n",
    "      tl.SRU(d_embedding),  # One RNN layer for conditional dependence.\n",
    "      tl.Dense(vocab_size),\n",
    "      tl.LogSoftmax()\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
