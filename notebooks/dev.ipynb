{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import math\n",
    "import jax\n",
    "import trax\n",
    "from trax import backend\n",
    "from trax import layers as tl\n",
    "from trax.backend import numpy as np\n",
    "from trax.layers.combinators import _pop_rng_and_split\n",
    "from trax.shapes import signature, ShapeDtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trax.models.research.reformer import ReformerLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 1000\n",
    "d_model=512,\n",
    "d_ff=2048,\n",
    "d_attention_key=64,\n",
    "d_attention_value=64,\n",
    "n_layers=6,\n",
    "n_heads=8,\n",
    "dropout=0.1,\n",
    "max_len=2048,\n",
    "n_chunks=0,\n",
    "n_attention_chunks=1,\n",
    "attention_type=tl.DotProductCausalAttention,\n",
    "share_qk=False,\n",
    "axial_pos_shape=(),\n",
    "d_axial_pos_embs=None,\n",
    "ff_activation=tl.FastGelu,\n",
    "ff_use_sru=0,\n",
    "mode='train'\n",
    "r = ReformerLM(vocab_size,\n",
    "               d_model=512,\n",
    "               d_ff=2048,\n",
    "               d_attention_key=64,\n",
    "               d_attention_value=64,\n",
    "               n_layers=6,\n",
    "               n_heads=8,\n",
    "               dropout=0.1,\n",
    "               max_len=2048,\n",
    "               n_chunks=0,\n",
    "               n_attention_chunks=1,\n",
    "               attention_type=tl.DotProductCausalAttention,\n",
    "               share_qk=False,\n",
    "               axial_pos_shape=(),\n",
    "               d_axial_pos_embs=None,\n",
    "               ff_activation=tl.FastGelu,\n",
    "               ff_use_sru=0,\n",
    "               mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.array([[0,1,2,3,4,5,6,7]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weights, state = r.init(input_signature)\n",
    "rng = backend.random.get_prng(0)\n",
    "inputs = (test, )*2\n",
    "output = r(inputs, weights=weights, state=state, rng=rng)\n",
    "dummy_loss = backend.numpy.sum(output[0])\n",
    "#r.forward_with_state(xs=(test,), weights=(1,)*r._n_layers, state=(1,)*r._n_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DeviceArray([[[-8.335339 , -8.690216 , -5.8988566, ..., -8.088729 ,\n",
       "                -8.524491 , -7.8314757],\n",
       "               [-8.316006 , -8.7872925, -6.1046233, ..., -8.088554 ,\n",
       "                -8.654272 , -7.870216 ],\n",
       "               [-8.455892 , -8.115887 , -5.906452 , ..., -8.373974 ,\n",
       "                -8.440801 , -7.525086 ],\n",
       "               ...,\n",
       "               [-8.354822 , -8.8492565, -6.0343447, ..., -6.980222 ,\n",
       "                -9.428055 , -6.6733527],\n",
       "               [-8.75255  , -9.481588 , -6.420909 , ..., -7.0931687,\n",
       "                -8.65214  , -5.918196 ],\n",
       "               [-8.388021 , -8.544396 , -6.143963 , ..., -7.09911  ,\n",
       "                -7.7136116, -6.223985 ]]], dtype=float32),\n",
       " array([[0, 1, 2, 3, 4, 5, 6, 7]]))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 8, 16), (1, 8, 16))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 16\n",
    "input_sd = ShapeDtype((1, 8), np.int32)\n",
    "input_signature = (input_sd, input_sd)\n",
    "model = ReformerLM(\n",
    "    vocab_size, d_model=32, d_ff=64,\n",
    "    d_attention_key=16, d_attention_value=16, n_layers=1, n_heads=2,\n",
    "    max_len=16, n_chunks=2, n_attention_chunks=1)\n",
    "final_shape = tl.check_shape_agreement(\n",
    "    model, input_signature)\n",
    "assert ((1, 8, 16), (1, 8, 16)) == final_shape\n",
    "final_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pMap(nn.Module):\n",
    "    \"\"\"Combinator for applying a layers to a list/tuple\"\"\"\n",
    "    def __init__(self, layer, n_sections=1, check_shapes=True):\n",
    "        \"\"\"\n",
    "        \n",
    "        :param layers: the layer you wish to apply to each element.\n",
    "        :param n_sections: number of sections to map to. defaults to 1.\n",
    "        :param check_shapes: tests to see the shapes are identical.\n",
    "        :returns: new layers with mapped layer to all elements.\n",
    "        \"\"\"\n",
    "        super(pMap, self).__init__()\n",
    "        if layer is None or isinstance(layer, (list, tuple)):\n",
    "            layer = nn.Sequential(*layer)\n",
    "        self.layer = layer\n",
    "        self.check_shapes = check_shapes\n",
    "        self.n_sections = n_sections\n",
    "        self.n_in = n_sections\n",
    "        self.n_out = n_sections\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs, **kwargs):\n",
    "        \"\"\"\n",
    "        Trying to replace with a basic forward pass. Trax implementation\n",
    "        uses a PRNG key split into subsections zipped with the inputs var\n",
    "        then returns a list of those forward passed subsections as results\n",
    "        \"\"\"\n",
    "            \n",
    "        if self.n_sections == 1:\n",
    "            results = self.layer(inputs, **kwargs)\n",
    "        \n",
    "        else:\n",
    "            results = [self.layer(x) for x in inputs] \n",
    "            \n",
    "        return results, self.layer.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pMap(\n",
       "  (layer): Sequential(\n",
       "    (0): LayerNorm((1,), eps=1, elementwise_affine=True)\n",
       "    (1): Linear(in_features=1, out_features=1, bias=True)\n",
       "    (2): LogSoftmax()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmap = pMap(layer=[\n",
    "    nn.LayerNorm(1,1), \n",
    "    nn.Linear(1,1),\n",
    "    nn.LogSoftmax()], n_sections=10)\n",
    "\n",
    "pmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n",
      "tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])\n"
     ]
    }
   ],
   "source": [
    "test = torch.tensor([0,1,2,3,4,5,6,7,8,9], dtype=torch.float)\n",
    "print(test.unsqueeze(0).shape)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([tensor([0.], grad_fn=<LogSoftmaxBackward>),\n",
       "  tensor([0.], grad_fn=<LogSoftmaxBackward>),\n",
       "  tensor([0.], grad_fn=<LogSoftmaxBackward>),\n",
       "  tensor([0.], grad_fn=<LogSoftmaxBackward>),\n",
       "  tensor([0.], grad_fn=<LogSoftmaxBackward>),\n",
       "  tensor([0.], grad_fn=<LogSoftmaxBackward>),\n",
       "  tensor([0.], grad_fn=<LogSoftmaxBackward>),\n",
       "  tensor([0.], grad_fn=<LogSoftmaxBackward>),\n",
       "  tensor([0.], grad_fn=<LogSoftmaxBackward>),\n",
       "  tensor([0.], grad_fn=<LogSoftmaxBackward>)],\n",
       " OrderedDict([('0.weight', tensor([1.])),\n",
       "              ('0.bias', tensor([0.])),\n",
       "              ('1.weight', tensor([[0.8039]])),\n",
       "              ('1.bias', tensor([0.5340]))]))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmap(inputs=test.view(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BroadcastedDropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pBroadcastedDropout(nn.Module):\n",
    "    \n",
    "    def __init__(self, rate=0.0, mode='train', broadcast_dims=(-2,)):\n",
    "        super(pBroadcastedDropout, self).__init__()\n",
    "        \n",
    "        self.rate = rate\n",
    "        if self.rate >= 1.0:\n",
    "            raise ValueError(f'Dropout rate ({self.rate}) must be lower than 1')\n",
    "        elif self.rate < 0:\n",
    "            raise ValueError(f'Dropout rate ({self.rate}) must be at least 0.0')\n",
    "        \n",
    "        self.broadcast_dims = broadcast_dims\n",
    "        self.mode = mode\n",
    "        \n",
    "    def forward(self, x: torch.tensor, **kwargs):\n",
    "        if self.mode == 'train' and self.rate > 0.0:\n",
    "            noise_shape = list(x.shape)\n",
    "            \n",
    "            for dim in self.broadcast_dims:\n",
    "                noise_shape[dim] = 1\n",
    "                \n",
    "            keep_prob = 1 - self.rate\n",
    "            keep = np.random.binomial(n=1, p=keep_prob, size=tuple(noise_shape))\n",
    "            keep = torch.tensor(keep)\n",
    "            multiplier = keep / keep_prob\n",
    "            return x * multiplier\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd = pBroadcastedDropout(mode='train', rate=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  1.2500,  2.5000,  3.7500,  5.0000,  6.2500,  7.5000,  8.7500,\n",
       "         10.0000, 11.2500]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bd(test.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple FeedForward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FeedForward(sample, \n",
    "                d_input: int, \n",
    "                d_output: int, \n",
    "                dropout: float, \n",
    "                activation: bool, \n",
    "                mode: str):\n",
    "    \"\"\"\n",
    "    Building a simple Feed Forward NN\n",
    "    :param sample: sample data used to set the LayerNorm dimensions.\n",
    "    :param d_input: model input dimension.\n",
    "    :param d_output: model output dimension.\n",
    "    :param dropout: amount of dropout to apply on range [0.0, 1.0).\n",
    "    :param activation: True applies ReLU activation.\n",
    "    :param mode: whether to run dropout in train or eval mode.\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(sample.shape) == 1:\n",
    "        norm_output = sample.shape[0]\n",
    "        norm_input = 1\n",
    "    elif len(sample.shape) == 2:\n",
    "        norm_output = sample.shape[1]\n",
    "        norm_input = sample.shape[0]\n",
    "    else:\n",
    "        norm_output = sample.shape[-1]\n",
    "        norm_input = sample.shape[-2]\n",
    "        \n",
    "    \n",
    "    return nn.Sequential(\n",
    "        nn.LayerNorm(normalized_shape=(norm_input, norm_output)),\n",
    "        nn.Linear(d_input, d_output),\n",
    "        pBroadcastedDropout(rate=dropout, mode=mode),\n",
    "        activation,\n",
    "        nn.Linear(d_output, d_output),\n",
    "        pBroadcastedDropout(rate=dropout, mode=mode)\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'jax.numpy' has no attribute 'random'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-549-ef8a2825f775>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m                  \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                  mode='train')\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-91-b8cad9f44475>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mkeep_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mkeep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mkeep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mmultiplier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeep\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/trax/backend.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'np'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'jax.numpy' has no attribute 'random'"
     ]
    }
   ],
   "source": [
    "ff = FeedForward(sample=test.view(1,-1), \n",
    "                 d_input=test.view(1,-1).shape[1], \n",
    "                 d_output=10, \n",
    "                 dropout=0.2, \n",
    "                 activation=nn.ReLU(), \n",
    "                 mode='train')\n",
    "ff(test.view(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitForOutput(nn.Module):\n",
    "    \"\"\"\n",
    "    Splits activations into sections, to be used prior to the output layer.\n",
    "    \n",
    "    After the reversible portion of the network, there is a final output portion that's \n",
    "    non-reversible where the minimum includes normalization, output projection, and log-softmax. \n",
    "    The output portion needs to operate on chucks of the sequence to avoid running out of memory\n",
    "    for large vocabulary sizes.\n",
    "    \n",
    "    This layer concatenates the two subparts of the activations along the feature dimension\n",
    "    then splits into chunks along the time dimension. \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_sections=2, axis=-2, n_in=2):\n",
    "        super(SplitForOutput, self).__init__()\n",
    "        self.n_sections = n_sections\n",
    "        self.axis = axis\n",
    "        self.n_in = 2\n",
    "        self.n_out = n_sections\n",
    "            \n",
    "    def forward(self, inputs: torch.tensor):\n",
    "                    \n",
    "        x1, x2 = inputs\n",
    "\n",
    "        x1_split = np.split(x1, self.n_sections, self.axis)\n",
    "        x2_split = np.split(x2, self.n_sections, self.axis)\n",
    "\n",
    "        res = [np.concatenate(ys, -1) for ys in zip(x1_split, x2_split)]\n",
    "        return tuple(res)\n",
    "\n",
    "    def reverse(self, output, **kwargs):\n",
    "        \n",
    "        x1_split = []\n",
    "        x2_split = []\n",
    "        for y in output:\n",
    "            y1, y2 = np.split(y, 2, -1)\n",
    "            x1_split.append(y1)\n",
    "            x2_split.append(y2)\n",
    "\n",
    "        x1 = np.concatenate(x1_split, self.axis)\n",
    "        x2 = np.concatenate(x2_split, self.axis)\n",
    "\n",
    "        return (x1, x2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 812,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chunk(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_sections=2):\n",
    "        super(Chunk, self).__init__()\n",
    "        self.n_sections = n_sections\n",
    "        \n",
    "    def forward(self, x):\n",
    "        assert x.shape[1] % n_sections == 0\n",
    "        return torch.cat(torch.chunk(x, chunks=n_sections, dim=-2))\n",
    "    \n",
    "class Unchunk(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_sections=2):\n",
    "        super(Unchunk, self).__init__()\n",
    "        self.n_sections = n_sections\n",
    "        \n",
    "    def forward(self, x):\n",
    "        assert x.shape[0] % self.n_sections == 0\n",
    "        return torch.cat(torch.chunk(x, chunks=self.n_sections, dim=-3), dim=-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Chunk(x, n_sections=2):\n",
    "    assert x.shape[1] % n_sections == 0\n",
    "    \n",
    "    return torch.cat(torch.chunk(x, chunks=n_sections, dim=-2))\n",
    "\n",
    "def Unchunk(x, n_sections=2):\n",
    "    assert x.shape[0] % n_sections == 0\n",
    "    \n",
    "    return torch.cat(torch.chunk(x, chunks=n_sections, dim=-3), dim=-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.tensor([[1,0,0,0],[0,1,0,0],\n",
    "                    [0,0,1,0], [0,0,0,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 0., 0., 0.],\n",
       "         [0., 1., 0., 0.]]), tensor([[0., 0., 1., 0.],\n",
       "         [0., 0., 0., 1.]]))"
      ]
     },
     "execution_count": 580,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.chunk(test, chunks=2, dim=-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0],\n",
       "        [0, 1, 0, 0],\n",
       "        [0, 0, 1, 0],\n",
       "        [0, 0, 0, 1]])"
      ]
     },
     "execution_count": 553,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compute_residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DeviceArray([[ 1.1745598 ,  0.07205822,  0.6006763 ,  0.7884164 ],\n",
       "              [-0.5550922 ,  1.0654054 ,  0.6392876 , -0.07618227],\n",
       "              [-0.10679559,  0.02530393,  0.9922953 ,  0.02808513],\n",
       "              [-0.27318668,  0.24871418,  0.8338994 ,  1.1428925 ]],            dtype=float32),\n",
       " array([[1, 0, 0, 0],\n",
       "        [0, 1, 0, 0],\n",
       "        [0, 0, 1, 0],\n",
       "        [0, 0, 0, 1]]))"
      ]
     },
     "execution_count": 554,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test = test.view(1,-1)\n",
    "\n",
    "ff = feed_forward(d_model=4, d_ff=4, dropout=0.2, activation=True, mode='train')\n",
    "\n",
    "residual_layers = ff\n",
    "compute_residual = tl.Serial(\n",
    "    tl.Parallel([], tl.Dup()),\n",
    "    tl.Swap(),\n",
    "    tl.Parallel(residual_layers, [], [])\n",
    ")\n",
    "\n",
    "layers = [compute_residual, tl.Parallel(tl.Add(), [])]\n",
    "compute_residual = tl.Serial(layers)\n",
    "\n",
    "input_sd = ShapeDtype(tuple(test.shape), np.int32)\n",
    "input_signature = (input_sd, input_sd)\n",
    "weights, state = compute_residual.init(input_signature)\n",
    "\n",
    "output, state = compute_residual(\n",
    "    x=(test.numpy(), )*2, \n",
    "    weights=weights,\n",
    "    state=state\n",
    ")\n",
    "\n",
    "output, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReversibleHalfResidual(nn.Module):\n",
    "    \n",
    "    def __init__(self, residual_layers: list):\n",
    "        super(ReversibleHalfResidual, self).__init__()\n",
    "        self.residual_layers = residual_layers\n",
    "        self.model = nn.Sequential(*residual_layers)\n",
    "        \n",
    "        \n",
    "    def compute_residual(self, inputs, **kwargs):\n",
    "        \"\"\"\n",
    "        Replicating the JAX class of the same name step by step.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # replicating the compute_residuals operation\n",
    "        output = self.model(inputs)\n",
    "        # replicating the tl.Add() operation\n",
    "        output += output\n",
    "        return output, self.model\n",
    "    \n",
    "    def reverse(self, inputs):\n",
    "        #inputs -= inputs\n",
    "        output = self.compute_residual(inputs)[0] - inputs\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 569,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = [[1,0,0,0], [0,1,0,0],\n",
    "       [0,0,1,0], [0,0,0,1]]\n",
    "test = torch.tensor(test, dtype=torch.float32)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeLU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (\n",
    "            1 + torch.tanh(math.sqrt(2 / math.pi) * (\n",
    "                x + 0.044715 * torch.pow(x, 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RevNetBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_in, d_out, dropout=0.1, lol=[]):\n",
    "        super(RevNetBlock, self).__init__()\n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "        self.dropout = dropout\n",
    "\n",
    "        layers = []\n",
    "        if lol == list():\n",
    "            layers.append(nn.LayerNorm((d_in,d_out)))\n",
    "            layers.append(nn.Linear(d_in, d_out))\n",
    "            layers.append(GeLU())\n",
    "            layers.append(nn.Linear(d_in, d_out))\n",
    "        else:\n",
    "            for layer in lol:\n",
    "                layers.append(layer)\n",
    "        \n",
    "        self.bottleneck_block = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.cat((x, x), dim=1)\n",
    "        x1, x2 = self.split(x)\n",
    "        Fx2 = self.bottleneck_block(x2)\n",
    "        y1 = Fx2 + x1\n",
    "        return (x2, y1)\n",
    "    \n",
    "    def inverse(self, x):\n",
    "        x2, y1 = x[0], x[1]\n",
    "        Fx2 = - self.bottleneck_block(x2)\n",
    "        x1 = Fx2 + y1\n",
    "        return (x1, x2)\n",
    "\n",
    "    @staticmethod\n",
    "    def split(x):\n",
    "        n = int(x.size()[1] / 2)\n",
    "        x1 = x[:, :n].contiguous()\n",
    "        x2 = x[:, n:].contiguous()\n",
    "        return (x1, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 0., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 0., 0., 1.]]), tensor([[ 1.4723,  0.0600,  0.5037,  0.5120],\n",
       "         [ 0.3461,  0.8839,  0.3724,  0.2543],\n",
       "         [ 0.7384,  0.1297,  1.1142,  0.3288],\n",
       "         [ 0.5839, -0.2881,  0.0728,  1.2639]], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 581,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb = RevNetBlock(d_in=4, d_out=4)\n",
    "rb(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1025,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RevNetHalfAttnBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_in, d_out, dropout=0.1, lol=[]):\n",
    "        super(RevNetHalfAttnBlock, self).__init__()\n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "        self.dropout = dropout\n",
    "\n",
    "        layers = []\n",
    "        if lol == list():\n",
    "            layers.append(nn.LayerNorm((d_in,d_out)))\n",
    "            layers.append(nn.Linear(d_out, d_out))\n",
    "            layers.append(GeLU())\n",
    "            layers.append(nn.Linear(d_out, d_out))\n",
    "        else:\n",
    "            for layer in lol:\n",
    "                layers.append(layer)\n",
    "        \n",
    "        self.bottleneck_block = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.cat((x, x), dim=1)\n",
    "        x1, x2 = self.split(x)\n",
    "        Fx2 = self.bottleneck_block(x2)\n",
    "        y1 = Fx2 + x1\n",
    "        return (x2, y1)\n",
    "    \n",
    "    def inverse(self, x):\n",
    "        x2, y1 = x[0], x[1]\n",
    "        Fx2 = - self.bottleneck_block(x2)\n",
    "        x1 = Fx2 + y1\n",
    "        return (x1, x2)\n",
    "\n",
    "    @staticmethod\n",
    "    def split(x):\n",
    "        n = int(x.size()[1] / 2)\n",
    "        x1 = x[:, :n].contiguous()\n",
    "        x2 = x[:, n:].contiguous()\n",
    "        return (x1, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReversibleHalfResidual(tl.ReversibleLayer, tl.Serial):\n",
    "    \"\"\"Half of a RevNet-style residual (only updates part of the hidden state).\"\"\"\n",
    "\n",
    "    def __init__(self, residual_layers):\n",
    "        self.compute_residual = tl.Serial(\n",
    "            # (x1_or_y1, x2) -> (x2, x1_or_y1, x2)\n",
    "            tl.Parallel([], tl.Dup()),\n",
    "            tl.Swap(),\n",
    "            tl.Parallel(residual_layers, [], []),\n",
    "        )\n",
    "\n",
    "        layers = [\n",
    "            self.compute_residual,\n",
    "            tl.Parallel(tl.Add(), [])\n",
    "        ]\n",
    "        super(ReversibleHalfResidual, self).__init__(layers)\n",
    "\n",
    "        self.subtract_top = tl.Parallel(tl.SubtractTop(), [])\n",
    "        self.reverse_layers = [self.compute_residual, self.subtract_top]\n",
    "\n",
    "    def reverse(self, output, weights=(), state=(), new_state=(), **kwargs):\n",
    "        reconstructed_x = output\n",
    "        rng = kwargs.pop('rng', None)\n",
    "        rngs = (None,) * self._n_layers\n",
    "        if rng is not None:\n",
    "            rngs = backend.random.split(rng, self._n_layers)\n",
    "        # Note that self.sublayers aligns exactly with self.reverse_layers in\n",
    "        # terms of parameter and rng usage, so no re-ordering is required.\n",
    "        for layer, p, s, ns, rng in zip(\n",
    "            self.reverse_layers, weights, state, new_state, rngs):\n",
    "            reconstructed_x = layer(reconstructed_x, weights=p,\n",
    "                                  state=s, new_state=ns, rng=rng, **kwargs)\n",
    "        return reconstructed_x\n",
    "\n",
    "    def reverse_and_grad(self, output, ct, weights=(), state=(), new_state=(),\n",
    "                       **kwargs):\n",
    "        rng = kwargs.pop('rng', None)\n",
    "        rngs = (None,) * self._n_layers\n",
    "        if rng is not None:\n",
    "            rngs = backend.random.split(rng, self._n_layers)\n",
    "\n",
    "        def call_compute_residual(x, weights):\n",
    "            res = self.compute_residual(x, weights=weights, state=state[0],\n",
    "                                  rng=rngs[0], **kwargs)\n",
    "            return res\n",
    "\n",
    "        assert len(ct) == 2\n",
    "        ct = ((ct[0], ct[0], ct[1]))\n",
    "\n",
    "        stack_with_residual, vjpfun = jax.vjp(\n",
    "            call_compute_residual, output, weights[0])\n",
    "        reconstructed_x = self.subtract_top(\n",
    "            stack_with_residual, weights=weights[-1], state=state[-1], rng=rngs[-1],\n",
    "            **kwargs)\n",
    "\n",
    "        x_ct, residual_weights_ct = vjpfun(ct)\n",
    "        assert not jax.tree_util.tree_leaves(weights[-1])\n",
    "        add_top_weights_ct = weights[-1]\n",
    "        return reconstructed_x, (x_ct, [residual_weights_ct, add_top_weights_ct])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfo = SplitForOutput()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.rand((4,4,64)).numpy()\n",
    "a, b = sfo.forward((t,t))\n",
    "c, d = sfo.reverse((t,t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 826,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComputeAttentionHeads(nn.Module):\n",
    "    def __init__(self, n_heads=1, d_head=64):\n",
    "        super(ComputeAttentionHeads, self).__init__()\n",
    "        self.n_heads= n_heads\n",
    "        self.d_head = d_head\n",
    "        \n",
    "    def forward(self, x):\n",
    "        seqlen = x.shape[1]\n",
    "        res = x\n",
    "        \n",
    "        # n_batch, seqlen, n_heads*d_head -> n_batch, seqlen, n_heads, d_head\n",
    "        res = np.reshape(res, (x.shape[0], seqlen, self.n_heads, self.d_head))\n",
    "        # n_batch, seqlen, n_heads, d_head -> n_batch, n_heads, seqlen, d_head\n",
    "        res = np.transpose(res, (0, 2, 1, 3))\n",
    "        # n_batch, n_heads, seqlen, d_head -> n_batch*n_heads, seqlen, d_head\n",
    "        res = np.reshape(res, (-1, seqlen, self.d_head))\n",
    "        res = nn.Linear(res.shape[-1], res.shape[-1])(res)\n",
    "        return res\n",
    "    \n",
    "class ComputeAttentionOutput(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ComputeAttentionOutput, self).__init__()\n",
    "        self.n_heads = 1\n",
    "        \n",
    "    def forward(self, x):\n",
    "        seqlen = x.shape[1]\n",
    "        d_head = x.shape[2]\n",
    "        \n",
    "        x = np.reshape(a=x, newshape=(-1, self.n_heads, seqlen, d_head))\n",
    "        x = np.reshape(x, (-1, self.n_heads, seqlen, d_head))\n",
    "        x = np.transpose(x, (0, 2, 1, 3))  # -> n_batch, seqlen, n_heads, d_head\n",
    "        x = np.reshape(x, (-1, seqlen, self.n_heads * d_head))\n",
    "        x = torch.tensor(x)\n",
    "        x = nn.Linear(x.shape[-1], x.shape[-1])(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1057,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 d_in, \n",
    "                 d_out, \n",
    "                 attn_k=64, \n",
    "                 attn_v=64, \n",
    "                 n_heads=1, \n",
    "                 n_chunks=2, \n",
    "                 share_qk=True, \n",
    "                 attn_type=None, \n",
    "                 dropout=None, \n",
    "                 ff_activation=None, \n",
    "                 ff_use_sru=None, \n",
    "                 mode='train'):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        \n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "        self.attn_k = attn_k\n",
    "        self.attn_v = attn_v\n",
    "        self.n_heads = n_heads\n",
    "        self.n_chunks = n_chunks\n",
    "        self.attn_type = attn_type\n",
    "        self.dropout = dropout\n",
    "        self.share_qk = share_qk\n",
    "        self.ff_activation = ff_activation\n",
    "        self.ff_use_sru = ff_use_sru\n",
    "        self.mode = mode\n",
    "        \n",
    "        \n",
    "    def pre_attention(self, x):\n",
    "        x1, x2 = torch.chunk(x, self.n_chunks)\n",
    "        #x1 = x2 = torch.cat(torch.chunk(x, chunks=self.n_chunks, dim=-2))\n",
    "        k_layers = [ComputeAttentionHeads(self.n_heads, self.attn_k), nn.LayerNorm((x.shape[1], x.shape[2]))]\n",
    "        k_model = nn.Sequential(*k_layers)\n",
    "\n",
    "        v_layers = [ComputeAttentionHeads(self.n_heads, self.attn_v), nn.LayerNorm((x.shape[1], x.shape[2]))]\n",
    "        v_model = nn.Sequential(*v_layers)\n",
    "\n",
    "        k = k_model(x1)\n",
    "        v = v_model(x2)\n",
    "\n",
    "        if not self.share_qk:\n",
    "            q_layers= k_layers\n",
    "            q_model = nn.Sequential(*q_layers)\n",
    "            q = q_model(x1)\n",
    "            \n",
    "            q = Fx1\n",
    "            \n",
    "            return (q, k, v)\n",
    "        else:\n",
    "            return (k, k, v)\n",
    "    \n",
    "    \n",
    "    def attention(self, inputs):\n",
    "        \n",
    "        assert len(inputs) == 2 or len(inputs) == 3\n",
    "        if len(inputs) == 2:\n",
    "            k, v = inputs\n",
    "            q = k\n",
    "        else:\n",
    "            q, k, v = inputs\n",
    "        \n",
    "        mask_size = q.shape[-2]\n",
    "        mask = torch.tril(\n",
    "            torch.ones((1, mask_size, mask_size), dtype=torch.bool), \n",
    "            diagonal=0\n",
    "        )\n",
    "        \n",
    "        attn = self.dotproductattention(q, k, v, mask)\n",
    "        return attn\n",
    "    \n",
    "    \n",
    "    def dotproductattention(self, q, k, v, mask, dropout=0.1):\n",
    "        \n",
    "        depth = q.shape[-1]\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) / np.sqrt(depth)\n",
    "        dots = F.log_softmax(torch.where(mask, dots, torch.full_like(dots, -1e9)), dim=0)\n",
    "        \n",
    "        keep_prob = 1 - dropout\n",
    "        keep = np.random.binomial(n=1, p=keep_prob, size=dots.shape)\n",
    "        \n",
    "        dots = torch.where(\n",
    "            torch.tensor(keep, dtype=torch.bool), \n",
    "            dots / torch.tensor(keep_prob), \n",
    "            torch.zeros_like(dots)\n",
    "        )\n",
    "        attn = torch.matmul(dots, v)\n",
    "        return attn\n",
    "    \n",
    "    \n",
    "    def post_attention(self, x):\n",
    "        \n",
    "        cao = ComputeAttentionOutput()\n",
    "        unchunk = Unchunk(n_sections = self.n_chunks)\n",
    "        bd = pBroadcastedDropout(rate=self.dropout)\n",
    "        \n",
    "        res = cao(x)\n",
    "        #res = torch.cat((res, res), dim=-3)\n",
    "        res = bd(res)\n",
    "        return res\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pre_attention(x)\n",
    "        x = tuple(torch.tensor(y) for y in x)\n",
    "        x = self.attention(x)\n",
    "        x = self.post_attention(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1067,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:106: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "db = DecoderBlock(d_in=10, d_out=10, dropout=0.1)\n",
    "q,k,v = db.pre_attention(torch.tensor(t))\n",
    "attn = db.attention((k,v))\n",
    "res = db.post_attention(torch.tensor(attn))\n",
    "res = db(torch.tensor(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1068,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4, 64])"
      ]
     },
     "execution_count": 1068,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb = RevNetHalfAttnBlock(d_in=res.shape[-2], d_out=res.shape[-1])\n",
    "output = rb(res)\n",
    "output = torch.cat(output)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1071,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.3375e+00,  7.5782e-01, -1.3530e+00,  ..., -4.8981e-01,\n",
       "           6.5929e-01,  2.1808e-01],\n",
       "         [-6.1448e+00,  2.0287e+00, -4.2357e+00,  ..., -1.6251e+00,\n",
       "           2.1483e+00,  1.1392e+00],\n",
       "         [-5.0683e+00,  1.7662e+00, -2.7701e+00,  ..., -1.2889e+00,\n",
       "           1.2900e+00,  7.0707e-01],\n",
       "         [-5.2799e+00,  1.8458e+00, -2.8506e+00,  ..., -1.3264e+00,\n",
       "           1.3344e+00,  7.0618e-01]],\n",
       "\n",
       "        [[-6.7761e+00,  4.8747e+00, -2.2219e+00,  ..., -1.0755e+00,\n",
       "           1.4934e+00, -9.0276e-01],\n",
       "         [-3.8855e+00,  2.2801e+00, -1.2036e+00,  ..., -1.2019e+00,\n",
       "           7.6750e-01, -1.4830e-03],\n",
       "         [-5.1901e+00,  3.2412e+00, -1.5614e+00,  ..., -1.1380e+00,\n",
       "           7.8082e-01, -3.8353e-01],\n",
       "         [-4.1313e+00,  2.7029e+00, -1.3616e+00,  ..., -8.7370e-01,\n",
       "           7.8209e-01, -2.3808e-01]],\n",
       "\n",
       "        [[-2.5208e+00,  5.6488e-01, -1.2040e+00,  ..., -4.2917e-01,\n",
       "           6.0526e-01,  1.7405e-01],\n",
       "         [-6.6358e+00,  1.6249e+00, -3.8189e+00,  ..., -1.5218e+00,\n",
       "           2.1253e+00,  1.2700e+00],\n",
       "         [-5.4280e+00,  1.4283e+00, -2.4754e+00,  ..., -1.2586e+00,\n",
       "           1.2222e+00,  7.1270e-01],\n",
       "         [-5.6560e+00,  1.4921e+00, -2.5397e+00,  ..., -1.2999e+00,\n",
       "           1.2686e+00,  7.1709e-01]],\n",
       "\n",
       "        [[-6.8105e+00,  4.5373e+00, -1.8237e+00,  ..., -8.9714e-01,\n",
       "           1.4173e+00, -1.3473e+00],\n",
       "         [-3.9387e+00,  2.0689e+00, -9.8096e-01,  ..., -1.0685e+00,\n",
       "           7.3832e-01, -2.4712e-01],\n",
       "         [-5.2214e+00,  2.9852e+00, -1.2674e+00,  ..., -9.8698e-01,\n",
       "           7.5564e-01, -6.9737e-01],\n",
       "         [-4.1668e+00,  2.4811e+00, -1.1302e+00,  ..., -7.3409e-01,\n",
       "           7.3762e-01, -5.1296e-01]]], grad_fn=<CatBackward>)"
      ]
     },
     "execution_count": 1071,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ReformerLM(vocab_size,\n",
    "               d_model=512,\n",
    "               d_ff=2048,\n",
    "               d_attention_key=64,\n",
    "               d_attention_value=64,\n",
    "               n_layers=6,\n",
    "               n_heads=8,\n",
    "               dropout=0.1,\n",
    "               max_len=2048,\n",
    "               n_chunks=0,\n",
    "               n_attention_chunks=1,\n",
    "               attention_type=tl.DotProductCausalAttention,\n",
    "               share_qk=False,\n",
    "               axial_pos_shape=(),\n",
    "               d_axial_pos_embs=None,\n",
    "               ff_activation=tl.FastGelu,\n",
    "               ff_use_sru=0,\n",
    "               mode='train'):\n",
    "  \"\"\"Reversible transformer language model (only uses a decoder, no encoder).\n",
    "  Args:\n",
    "    vocab_size: int: vocab size\n",
    "    d_model: int:  depth of *each half* of the two-part features\n",
    "    d_ff: int: depth of feed-forward layer\n",
    "    d_attention_key: int: depth of key vector for each attention head\n",
    "    d_attention_value: int: depth of value vector for each attention head\n",
    "    n_layers: int: number of decoder layers\n",
    "    n_heads: int: number of attention heads\n",
    "    dropout: float: dropout rate (how much to drop out)\n",
    "    max_len: int: maximum symbol length for positional encoding\n",
    "    n_chunks: int: number of chunks (must match input pipeline)\n",
    "    n_attention_chunks: int: number of chunks for attention\n",
    "    attention_type: class: attention class to use, such as DotProductAttention.\n",
    "    share_qk: bool, whether to share queries and keys.\n",
    "    axial_pos_shape: tuple of ints: input shape to use for the axial position\n",
    "      encoding. If unset, axial position encoding is disabled.\n",
    "    d_axial_pos_embs: tuple of ints: depth of position embedding for each axis.\n",
    "      Tuple length must match axial_pos_shape, and values must sum to d_model.\n",
    "    ff_activation: the non-linearity in feed-forward layer\n",
    "    ff_use_sru: int; if > 0, we use this many SRU layers instead of feed-forward\n",
    "    mode: str: 'train', 'eval', or 'predict'\n",
    "  Returns:\n",
    "    the layer.\n",
    "  \"\"\"\n",
    "  if n_chunks == 0:\n",
    "    n_chunks = 1\n",
    "    concatenate_input_chunks = []\n",
    "  else:\n",
    "    concatenate_input_chunks = tl.Concatenate(n_items=n_chunks)\n",
    "\n",
    "  if not axial_pos_shape:\n",
    "    positional_encoding = tl.PositionalEncoding(\n",
    "        max_len=max_len, dropout=dropout, mode=mode)\n",
    "  else:\n",
    "    assert d_axial_pos_embs is not None\n",
    "    positional_encoding = tl.AxialPositionalEncoding(\n",
    "        shape=axial_pos_shape, d_embs=d_axial_pos_embs,\n",
    "        dropout_broadcast_dims=tuple(range(1, len(axial_pos_shape) + 1)),\n",
    "        dropout=dropout, mode=mode)\n",
    "\n",
    "  positional_embedder = [\n",
    "      tl.Embedding(d_model, vocab_size),\n",
    "      BroadcastedDropout(rate=dropout, mode=mode),  # pylint: disable=no-value-for-parameter\n",
    "      positional_encoding,\n",
    "  ]\n",
    "\n",
    "  decoder_blocks = []\n",
    "\n",
    "  if isinstance(attention_type, (tuple, list)):\n",
    "    assert n_layers % len(attention_type) == 0\n",
    "  else:\n",
    "    attention_type = [attention_type]\n",
    "  for layer_idx in range(n_layers):\n",
    "    layer_attention_type = attention_type[layer_idx % len(attention_type)]\n",
    "    decoder_block = DecoderBlock(\n",
    "        d_model, d_ff, d_attention_key, d_attention_value, n_heads,\n",
    "        n_attention_chunks,\n",
    "        attention_type=layer_attention_type,\n",
    "        dropout=dropout,\n",
    "        share_qk=(share_qk or issubclass(layer_attention_type,\n",
    "                                         tl.LSHCausalAttention)),\n",
    "        ff_activation=ff_activation,\n",
    "        ff_use_sru=ff_use_sru,\n",
    "        mode=mode)\n",
    "    decoder_blocks.append(decoder_block)\n",
    "\n",
    "  return tl.Serial(\n",
    "      concatenate_input_chunks,\n",
    "      tl.ShiftRight(mode=mode),\n",
    "      positional_embedder,\n",
    "      tl.Dup(),\n",
    "      tl.ReversibleSerial(decoder_blocks + [\n",
    "          SplitForOutput(n_sections=n_chunks, axis=-2),  # pylint: disable=no-value-for-parameter\n",
    "      ]),\n",
    "      Map([\n",
    "          # TODO(kitaev): Test whether dropout should go before or after the\n",
    "          # LayerNorm, and whether dropout broadcasting is needed here.\n",
    "          tl.LayerNorm(),\n",
    "          BroadcastedDropout(rate=dropout, mode=mode),  # pylint: disable=no-value-for-parameter\n",
    "          tl.Dense(vocab_size),\n",
    "          tl.LogSoftmax(),\n",
    "      ], n_sections=n_chunks),\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ReformerShortenLM(vocab_size,\n",
    "                      shorten_factor=1,\n",
    "                      d_embedding=256,\n",
    "                      d_model=512,\n",
    "                      d_ff=2048,\n",
    "                      d_attention_key=64,\n",
    "                      d_attention_value=64,\n",
    "                      n_layers=6,\n",
    "                      n_heads=8,\n",
    "                      dropout=0.1,\n",
    "                      max_len=2048,\n",
    "                      n_attention_chunks=1,\n",
    "                      attention_type=tl.DotProductCausalAttention,\n",
    "                      share_qk=False,\n",
    "                      axial_pos_shape=(),\n",
    "                      d_axial_pos_embs=None,\n",
    "                      ff_activation=tl.FastGelu,\n",
    "                      ff_use_sru=0,\n",
    "                      mode='train'):\n",
    "  \"\"\"Reversible transformer language model with shortening.\n",
    "  When shorten_factor is F and processing an input of shape [batch, length],\n",
    "  we embed the (shifted-right) input and then group each F elements (on length)\n",
    "  into a single vector -- so that in the end we process a tensor of shape\n",
    "    [batch, length // F, d_model]\n",
    "  almost until the end -- at the end it's un-shortend and a SRU is applied.\n",
    "  This reduces the length processed inside the main model body, effectively\n",
    "  making the model faster but possibly slightly less accurate.\n",
    "  Args:\n",
    "    vocab_size: int: vocab size\n",
    "    shorten_factor: by how much to shorten, see above\n",
    "    d_embedding: the depth of the embedding layer and final logits\n",
    "    d_model: int:  depth of *each half* of the two-part features\n",
    "    d_ff: int: depth of feed-forward layer\n",
    "    d_attention_key: int: depth of key vector for each attention head\n",
    "    d_attention_value: int: depth of value vector for each attention head\n",
    "    n_layers: int: number of decoder layers\n",
    "    n_heads: int: number of attention heads\n",
    "    dropout: float: dropout rate (how much to drop out)\n",
    "    max_len: int: maximum symbol length for positional encoding\n",
    "    n_attention_chunks: int: number of chunks for attention\n",
    "    attention_type: class: attention class to use, such as DotProductAttention.\n",
    "    share_qk: bool, whether to share queries and keys.\n",
    "    axial_pos_shape: tuple of ints: input shape to use for the axial position\n",
    "      encoding. If unset, axial position encoding is disabled.\n",
    "    d_axial_pos_embs: tuple of ints: depth of position embedding for each axis.\n",
    "      Tuple length must match axial_pos_shape, values must sum to d_embedding.\n",
    "    ff_activation: the non-linearity in feed-forward layer\n",
    "    ff_use_sru: int; if > 0, we use this many SRU layers instead of feed-forward\n",
    "    mode: str: 'train' or 'eval'\n",
    "  Returns:\n",
    "    the layer.\n",
    "  \"\"\"\n",
    "  assert mode != 'predict'  # TODO(lukaszkaiser,kitaev): fast inference\n",
    "\n",
    "  if not axial_pos_shape:\n",
    "    positional_encoding = tl.PositionalEncoding(\n",
    "        max_len=max_len, dropout=dropout, mode=mode)\n",
    "  else:\n",
    "    assert d_axial_pos_embs is not None\n",
    "    positional_encoding = tl.AxialPositionalEncoding(\n",
    "        shape=axial_pos_shape, d_embs=d_axial_pos_embs,\n",
    "        dropout_broadcast_dims=tuple(range(1, len(axial_pos_shape) + 1)),\n",
    "        dropout=dropout, mode=mode)\n",
    "\n",
    "  positional_embedder = [\n",
    "      tl.Embedding(d_embedding, vocab_size),\n",
    "      BroadcastedDropout(rate=dropout, mode=mode),  # pylint: disable=no-value-for-parameter\n",
    "      positional_encoding,\n",
    "  ]\n",
    "\n",
    "  decoder_blocks = []\n",
    "\n",
    "  if isinstance(attention_type, (tuple, list)):\n",
    "    assert n_layers % len(attention_type) == 0\n",
    "  else:\n",
    "    attention_type = [attention_type]\n",
    "  for layer_idx in range(n_layers):\n",
    "    layer_attention_type = attention_type[layer_idx % len(attention_type)]\n",
    "    decoder_block = DecoderBlock(\n",
    "        d_model, d_ff, d_attention_key, d_attention_value, n_heads,\n",
    "        n_attention_chunks,\n",
    "        attention_type=layer_attention_type,\n",
    "        dropout=dropout,\n",
    "        share_qk=(share_qk or issubclass(layer_attention_type,\n",
    "                                         tl.LSHCausalAttention)),\n",
    "        ff_activation=ff_activation,\n",
    "        ff_use_sru=ff_use_sru,\n",
    "        mode=mode)\n",
    "    decoder_blocks.append(decoder_block)\n",
    "\n",
    "  # pylint: disable=g-long-lambda\n",
    "  return tl.Serial(\n",
    "      tl.ShiftRight(),\n",
    "      positional_embedder,\n",
    "      tl.Dup(),              # Stack has (x, x), the first will be shortened\n",
    "      # Before shortening, we need to pad by shorten factor so as not to leak\n",
    "      # information into the future. To understand why, imagine shorten factor\n",
    "      # of 2 and sequence of length 4, so ABCD. If we shift just by 1, then we\n",
    "      # would have 0ABC, which gets grouped to [0A][BC] on input, which is\n",
    "      # predicting ABCD as targets. The problem is that [0A] has access to A\n",
    "      # and [BC] has access to C -- it will learn to copy it, peek into\n",
    "      # the future. Shifting twice to [00][AB] solves the problem as the first\n",
    "      # \"big\" symbol becomes all-0 and the rest is shifted enough.\n",
    "      tl.ShiftRight(n_shifts=shorten_factor - 1),\n",
    "      tl.Fn(lambda x: np.reshape(  # Shorten -- move to depth.\n",
    "          x, (x.shape[0], x.shape[1] // shorten_factor, -1)), n_out=1),\n",
    "      tl.Dense(d_model),\n",
    "      tl.Dup(),  # Stack has (short_x, short_x, x)\n",
    "      tl.ReversibleSerial(decoder_blocks),\n",
    "      tl.Select([0], n_in=2),\n",
    "      tl.LayerNorm(),\n",
    "      BroadcastedDropout(rate=dropout, mode=mode),  # pylint: disable=no-value-for-parameter\n",
    "      tl.Dense(shorten_factor * d_embedding),\n",
    "      tl.Fn(lambda x: np.reshape(  # Prolong back.\n",
    "          x, (x.shape[0], x.shape[1] * shorten_factor, -1)), n_out=1),\n",
    "      tl.Concatenate(),  # Concatenate with just the embeddings.\n",
    "      tl.CausalConv(d_embedding),\n",
    "      tl.Relu(),\n",
    "      tl.SRU(d_embedding),  # One RNN layer for conditional dependence.\n",
    "      tl.Dense(vocab_size),\n",
    "      tl.LogSoftmax()\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
